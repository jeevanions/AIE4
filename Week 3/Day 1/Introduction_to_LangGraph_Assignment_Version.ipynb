{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "  - ü§ù Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KaVwN269EttM"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "42b2ba5e-11ae-4f4d-e68e-77607bb794ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "30aa2260-50f1-4abf-b305-eed0ee1b9008"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE4 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method.\n",
        "\n",
        "We are using `duckduckgo_search` and `arxiv_query`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "duckduckgo_search = DuckDuckGoSearchRun()\n",
        "arxiv_query = ArxivQueryRun()\n",
        "\n",
        "tool_belt = [\n",
        "   duckduckgo_search,\n",
        "   arxiv_query\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Answer\n",
        "When there are multiple tools available, the model can use a routing mechanism called call_tools, which takes the output from the model (usually a JSON or similar structured output) and decides which tool to invoke based on this data.\n",
        "In addition that the model is provided with a list of tools available with their description \n",
        "\n",
        "```python\n",
        "# Description for the arxiv tool\n",
        "description: str = (\n",
        "        \"A wrapper around Arxiv.org \"\n",
        "        \"Useful for when you need to answer questions about Physics, Mathematics, \"\n",
        "        \"Computer Science, Quantitative Biology, Quantitative Finance, Statistics, \"\n",
        "        \"Electrical Engineering, and Economics \"\n",
        "        \"from scientific articles on arxiv.org. \"\n",
        "        \"Input should be a search query.\"\n",
        "    )\n",
        "\n",
        "# Description for the duckduck tool\n",
        "description: str = (\n",
        "        \"A wrapper around DuckDuckGo Search. \"\n",
        "        \"Useful for when you need to answer questions about current events. \"\n",
        "        \"Input should be a search query.\"\n",
        "    )\n",
        "    \n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "compiled_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "# Answer\n",
        "\n",
        "There is no predefined limit on the number of iterations or cycles. However, to prevent the possibility of an infinite loop, you can implement a counter that tracks the number of iterations. By adding a condition to this counter, you can specify a maximum threshold. If the counter exceeds this threshold, the loop is terminated, effectively imposing a limit on the number of cycles.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "a5d7ef7a-13f2-4066-df52-b0df89eae2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_OU2YynDHygr54Mnd73G68rEl', 'function': {'arguments': '{\"query\":\"ReAct: Synergizing Reasoning and Acting in Language Models\"}', 'name': 'arxiv'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 168, 'total_tokens': 194}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7f3fc5b5-ab42-4e30-864f-7c252912224f-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'ReAct: Synergizing Reasoning and Acting in Language Models'}, 'id': 'call_OU2YynDHygr54Mnd73G68rEl', 'type': 'tool_call'}], usage_metadata={'input_tokens': 168, 'output_tokens': 26, 'total_tokens': 194})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='Published: 2023-03-10\\nTitle: ReAct: Synergizing Reasoning and Acting in Language Models\\nAuthors: Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao\\nSummary: While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or two in-context examples.\\nProject site with code: https://react-lm.github.io\\n\\nPublished: 2024-08-12\\nTitle: Towards Autonomous Agents: Adaptive-planning, Reasoning, and Acting in Language Models\\nAuthors: Yen-Che Hsiao, Abhishek Dutta\\nSummary: We propose a novel in-context learning algorithm for building autonomous\\ndecision-making language agents. The language agent continuously attempts to\\nsolve the same task by self-correcting each time the task fails. Our selected\\nlanguage agent demonstrates the ability to solve tasks in a text-based game\\nenvironment. Our results show that the gemma-2-9b-it language model, using our\\nproposed method, can successfully complete two of six tasks that failed in the\\nfirst attempt. This highlights the effectiveness of our approach in enhancing\\nthe problem-solving capabilities of a single language model through\\nself-correction, paving the way for more advanced autonomous agents. The code\\nis publicly available at\\nhttps://github.com/YenCheHsiao/AutonomousLLMAgentwithAdaptingPlanning.\\n\\nPublished: 2021-09-03\\nTitle: CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation\\nAuthors: Wenchang Ma, Ryuichi Takanobu, Minlie Huang\\nSummary: Growing interests have been attracted in Conversational Recommender Systems\\n(CRS), which explore user preference through conversational interactions in\\norder to make appropriate recommendation. However, there is still a lack of\\nability in existing CRS to (1) traverse multiple reasoning paths over\\nbackground knowledge to introduce relevant items and attributes, and (2)\\narrange selected entities appropriately under current system intents to control\\nresponse generation. To address these issues, we propose CR-Walker in this\\npaper, a model that performs tree-structured reasoning on a knowledge graph,\\nand generates informative dialog acts to guide language generation. The unique\\nscheme of tree-structured reasoning views the traversed entity at each hop as\\npart of dialog acts to facilitate language generation, which links how entities\\nare selected and expressed. Automatic and human evaluations show that CR-Walker\\ncan arrive at more accurate recommendation, and generate more informative and\\nengaging responses.', name='arxiv', tool_call_id='call_OU2YynDHygr54Mnd73G68rEl')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='The authors of the paper \"ReAct: Synergizing Reasoning and Acting in Language Models\" are:\\n\\n- Shunyu Yao\\n- Jeffrey Zhao\\n- Dian Yu\\n- Nan Du\\n- Izhak Shafran\\n- Karthik Narasimhan\\n- Yuan Cao\\n\\nYou can find more details and access the project site with code [here](https://react-lm.github.io).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 1033, 'total_tokens': 1118}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-0cf1ffbb-4f23-4e82-95e7-916b778725dd-0', usage_metadata={'input_tokens': 1033, 'output_tokens': 85, 'total_tokens': 1118})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "# inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who are the author of 'ReAct: Synergizing Reasoning and Acting in Language Models' paper?\")]}\n",
        "\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "026a3aa3-1c3e-4016-b0a3-fe98b1d25399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_cQB76mjO1IYlvWwB6p16wv8c', 'function': {'arguments': '{\"query\": \"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_xf55Gm8ZTKPJ2Hsuc3WM14Ht', 'function': {'arguments': '{\"query\": \"latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 173, 'total_tokens': 223}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-ec18a8be-9e5f-4e50-a8f1-838508575b9d-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_cQB76mjO1IYlvWwB6p16wv8c', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'latest Tweet'}, 'id': 'call_xf55Gm8ZTKPJ2Hsuc3WM14Ht', 'type': 'tool_call'}], usage_metadata={'input_tokens': 173, 'output_tokens': 50, 'total_tokens': 223})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w', name='arxiv', tool_call_id='call_cQB76mjO1IYlvWwB6p16wv8c'), ToolMessage(content='New York\\'s iconic skyline and vibrant culture inspire me every day,\" the former first lady wrote Tuesday on X (formerly Twitter). Advertisement Her X account is only lightly active, posting from time to time to mark holidays, sell products and share statements. The usually quiet former first lady sent out a really random tweet that is gaining some attention. It\\'s this one about NYC: New York City captivated my heart the moment I arrived, 28 years ago today. 5. Me: I hate this escape room. Mom: Stop calling the family reunion that. ‚Äî Funny Snarky Humor (@FUNNYsnarkyJOKE) July 10, 2024. Twitter: @FUNNYsnarkyJOKE. Twitter Blue\\'s Chaotic Revamp: The overhaul of the subscription service that lets users buy verified badges was the first big test for Musk as the platform\\'s new owner. It didn\\'t go well . Sept. 15, 2023. The federal prosecutors who charged former President Donald J. Trump with a criminal conspiracy over his attempts to overturn the 2020 election obtained 32 private messages from ...', name='duckduckgo_search', tool_call_id='call_xf55Gm8ZTKPJ2Hsuc3WM14Ht')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Oq4oiziArWYnEqiezG1mecQ8', 'function': {'arguments': '{\"query\": \"Tim Dettmers latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_YUmXQzwRnrTznQc7L7RfIr8M', 'function': {'arguments': '{\"query\": \"Artidoro Pagnoni latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_AJzQWoaYO6jWpPCFZaEY1yiT', 'function': {'arguments': '{\"query\": \"Ari Holtzman latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_JjhzNvFPWgB6624ThaXqMfi2', 'function': {'arguments': '{\"query\": \"Luke Zettlemoyer latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 1428, 'total_tokens': 1536}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-fd0d5785-38f7-4ce2-bc10-47bb7daf89d3-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers latest Tweet'}, 'id': 'call_Oq4oiziArWYnEqiezG1mecQ8', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Artidoro Pagnoni latest Tweet'}, 'id': 'call_YUmXQzwRnrTznQc7L7RfIr8M', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Ari Holtzman latest Tweet'}, 'id': 'call_AJzQWoaYO6jWpPCFZaEY1yiT', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Luke Zettlemoyer latest Tweet'}, 'id': 'call_JjhzNvFPWgB6624ThaXqMfi2', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1428, 'output_tokens': 108, 'total_tokens': 1536})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: duckduckgo_search\n",
            "[ToolMessage(content='Former national security adviser H.R. McMaster said on Sunday he \"would never disparage\" someone\\'s military service, when asked about the controversy surrounding the Democratic vice ... Essentially, a CPU is a latency-optimized device while GPUs are bandwidth-optimized devices. If a CPU is a race car, a GPU is a cargo truck. The main job in deep learning is to fetch and move ... Her new position hasn\\'t been finalized. ... ‚Äî Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and ... Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... Tech Moves: AI researcher Yejin Choi leaves Univ. of Washington and Allen Institute for AI. by Todd Bishop & Taylor Soper on August 2, 2024. Yejin Choi, who was named a 2022 MacArthur Fellow and ...', name='duckduckgo_search', tool_call_id='call_Oq4oiziArWYnEqiezG1mecQ8'), ToolMessage(content=\"We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly ... efficient finetuning of quantized LLMs. AUTHORs: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer Authors Info & Claims. NIPS '23: Proceedings of the 37th International Conference on Neural Information Processing Systems. Article No.: 441, Pages 10088 - 10115. Published: 30 May 2024 Publication History. In this paper, we address these aforementioned challenges associated with financial data and introduce FinGPT, an end-to-end open-source framework for financial large language models (FinLLMs). Adopting a data-centric approach, FinGPT underscores the crucial role of data acquisition, cleaning, and preprocessing in developing open-source FinLLMs. Pagnoni et al. (2021) Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages ... Fine-tuning large language models (LLMs) is a common practice to adapt them for specific tasks, but it can be computationally expensive. LoRA (Low-Rank Adaptation) is a technique that makes this process more efficient by introducing small adapter modules to the model. These adapters capture task-specific knowledge without modifying the original ...\", name='duckduckgo_search', tool_call_id='call_YUmXQzwRnrTznQc7L7RfIr8M'), ToolMessage(content='MSNBC host Ari Melber, during an interview with Trump campaign adviser Corey Lewandowski on Wednesday, threatened him with a defamation lawsuit for quoting the anchor calling the former President ... Ari Melber warns Trump campaign advisor Corey Lewandowski he \\'will be potentially in a defamation situation\\' on Wednesday\\'s episode of The Beat (MSNBC) \"I did not say that. That is a false ... The heated on-air dispute last night between MSNBC\\'s Ari Melber and Donald Trump campaign adviser Corey Lewandowski didn\\'t end with Wednesday\\'s segment: Today, Lewandowski tweeted a video in ... The team behind QLoRA includes Allen School Ph.D. student Artidoro Pagnoni; alum Ari Holtzman (Ph.D., \\'23), incoming professor at the University of Chicago; and professor Luke Zettlemoyer, who is also a research manager at Meta. Madrona Prize First Runner Up / Punica: Multi-Tenant LoRA Fine-tuned LLM Serving The Astros are set to recall catcher Cesar Salazar from Triple-A Sugar Land for the September 1 roster expansion, allowing more flexibility for main catchers Yainer Diaz and Victor Caratini.', name='duckduckgo_search', tool_call_id='call_AJzQWoaYO6jWpPCFZaEY1yiT'), ToolMessage(content=\"Luke Zettlemoyer is a research manager and site lead for FAIR Seattle. He is also a Professor in the Allen School of Computer Science & Engineering at the University of Washington. His research is in empirical computational semantics, where the goal is to build models that recover representations of the meaning of natural language text. We introduce MoMa, a novel modality-aware mixture-of-experts (MoE) architecture designed for pre-training mixed-modal, early-fusion language models. MoMa processes images and text in arbitrary sequences by dividing expert modules into modality-specific groups. These groups exclusively process designated tokens while employing learned routing within each group to maintain semantically informed ... Luke McNally's time in the Championship hasn't exactly gone according to plan since he left Oxford United 2022, but that doesn't mean Bristol City's new defender hasn't produced a strong ... \\ufeff Twitter \\ufeff Reddit. Join our list for notifications and early access to events ... About this Episode. Today we're joined by Luke Zettlemoyer, professor at University of Washington and a research manager at Meta. In our conversation with Luke, we cover multimodal generative AI, the effect of data on models, and the significance of open ... Provost Tricia Serio and President Ana Mari Cauce have appointed a Task Force on Artificial Intelligence to address these issues and to suggest a UW-wide AI strategy. Chaired by Andreas Bohman, vice president of UW-IT and the University's chief information officer, and Anind Dey, dean of the Information School, the task force will initially ...\", name='duckduckgo_search', tool_call_id='call_JjhzNvFPWgB6624ThaXqMfi2')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"Here are the latest updates and tweets from the authors of the QLoRA paper:\\n\\n### Tim Dettmers\\n- **Latest Update**: Tim Dettmers is joining Ai2 as an AI researcher. He specializes in efficient deep learning at the intersection of machine learning, NLP, and more.\\n- **Latest Tweet**: No specific tweet found, but he is active in the AI research community.\\n\\n### Artidoro Pagnoni\\n- **Latest Update**: Artidoro Pagnoni is involved in research on efficient finetuning of quantized LLMs and has published several papers on the topic.\\n- **Latest Tweet**: No specific tweet found, but he is active in the AI research community.\\n\\n### Ari Holtzman\\n- **Latest Update**: Ari Holtzman is an alum of the Allen School (Ph.D., '23) and is an incoming professor at the University of Chicago.\\n- **Latest Tweet**: No specific tweet found, but he is active in the AI research community.\\n\\n### Luke Zettlemoyer\\n- **Latest Update**: Luke Zettlemoyer is a research manager and site lead for FAIR Seattle. He is also a Professor in the Allen School of Computer Science & Engineering at the University of Washington.\\n- **Latest Tweet**: No specific tweet found, but he is active in the AI research community.\\n\\nIt appears that while these authors are active in their respective fields, specific recent tweets were not found. They are, however, contributing significantly to AI research and development.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 312, 'prompt_tokens': 2759, 'total_tokens': 3071}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-8cb7b828-5541-4290-ab38-de48ac09ceb5-0', usage_metadata={'input_tokens': 2759, 'output_tokens': 312, 'total_tokens': 3071})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        print(values[\"messages\"])\n",
        "\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Answer\n",
        "1. User asks the question \"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\"  This prompt is explicit in giving instruction to LLM model to use the tools from the toolbelt.\n",
        "2. We start the compiled graph using an async `astream` function passing in the input.\n",
        "3. As `agent` is set as the entrypoint in the graph and the input (state object) is sent to the `agent`\n",
        "4. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "5. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node. In the response we can see five `ToolMessage` and explained below in order\n",
        "   1. On close inspection the first `ToolMessage` is the response from the `arxiv` tool providing the Paper on QLora with the authors. Note that there 4 authors for this paper.\n",
        "   2. Next 4 `ToolMessage` are the response from the tool `duckduckgo_search` one for each author finding the latest tweets. Important point to note it is not clear which Author' latest tweets are in the tool reponse.\n",
        "7. The agent node added a response to the state object and passed it along the conditional edge\n",
        "8. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | compiled_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "b160f3e4-89d1-4411-ed96-fe17b3cad200"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) that combines retrieval-based methods with generative models to improve the quality and relevance of generated text. Here's a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus based on the input query. This is typically done using a retrieval model like BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input query. This means that the information from the retrieved documents is combined with the original query to provide more context and relevant information.\\n\\n3. **Generation**: Finally, a generative model, such as a transformer-based model like GPT-3, uses the augmented input to generate a response. The additional context from the retrieved documents helps the generative model produce more accurate and contextually relevant responses.\\n\\nRAG is particularly useful in scenarios where the generative model alone might not have enough information to produce a high-quality response, such as in open-domain question answering or conversational AI. By leveraging external knowledge through retrieval, RAG can significantly enhance the performance of generative models.\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\",\n",
        "    \"What is the primary goal of the QLoRA method?\",\n",
        "    \"What are the main components of the QLoRA approach?\",\n",
        "    \"What datasets were used to evaluate the QLoRA method?\",\n",
        "    \"Who are the primary authors of the QLoRA paper?\",\n",
        "    \"What models are fine-tuned using the QLoRA method?\",\n",
        "    \"How does QLoRA achieve efficiency in fine-tuning?\",\n",
        "    \"What are the key results reported in the QLoRA paper?\",\n",
        "    \"What limitations of QLoRA are discussed in the paper?\",\n",
        "    \"How does QLoRA differ from other fine-tuning methods?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\": [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\": [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\": [\"ground\", \"context\"]},\n",
        "    {\"must_mention\": [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\": [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\": [\"reduce\", \"parameters\"]},\n",
        "    {\"must_mention\": [\"efficient\", \"fine-tuning\"]},\n",
        "    {\"must_mention\": [\"low-rank\", \"adapters\", \"quantization\"]},\n",
        "    {\"must_mention\": [\"datasets\", \"evaluation\"]},\n",
        "    {\"must_mention\": [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\": [\"transformer\", \"models\", \"GPT-3\", \"BERT\"]},\n",
        "    {\"must_mention\": [\"quantization\", \"efficiency\"]},\n",
        "    {\"must_mention\": [\"results\", \"benchmarks\", \"performance\"]},\n",
        "    {\"must_mention\": [\"limitations\", \"future work\"]},\n",
        "    {\"must_mention\": [\"contrast\", \"methods\", \"fine-tuning\"]}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "examples = [{\"question\": q, \"answer\": a} for q, a in zip(questions, answers)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "# client.create_examples(\n",
        "#     inputs=[{\"question\" : q} for q in questions],\n",
        "#     outputs=answers,\n",
        "#     dataset_id=dataset.id,\n",
        "# )\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\": qa[\"question\"]} for qa in examples],\n",
        "    outputs=[{\"answer\": qa[\"answer\"]} for qa in examples],\n",
        "    dataset_id=dataset.id,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "\n",
        "1. As per this create example; in python the list are are ordered and it will be fed to this function in the order they are specified. The problem with this approach is that it can go out of sequence if an item is added or removed without updating the other list. It would be better to use different programming construct like dictionory in a pair of question and answer. \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "1. We are using key words which should be present in the output to determine the LLM output is valid or not. Also the LLM generate the response which is accurate but does not have the words we expect to be present. Presence of the keyword doesn't mean that it is relevant and accurate. \n",
        "2. I would compare the semantic meaning of the response with these key words and determine whther they colosets to each other.  I added custom evaluator which uses semantic comparison in addition to the key work match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "import numpy as np\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "@run_evaluator\n",
        "def semantic_match(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required_phrases = example.outputs.get(\"must_mention\") or []\n",
        "    \n",
        "    prediction_embedding = embedding_model.embed_query(prediction)\n",
        "    \n",
        "    required_embeddings = [embedding_model.embed_query(phrase) for phrase in required_phrases]\n",
        "\n",
        "    prediction_embedding = np.array(prediction_embedding)\n",
        "    required_embeddings = np.array(required_embeddings)\n",
        "    \n",
        "    # Compute cosine similarity\n",
        "    def cosine_similarity(a, b):\n",
        "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "    \n",
        "    # Check similarity for each required phrase\n",
        "    similarity_threshold = 0.7\n",
        "    similarities = [cosine_similarity(prediction_embedding, req_emb) for req_emb in required_embeddings]\n",
        "    \n",
        "    # Determine if all required phrases have similarity above the threshold\n",
        "    score = all(similarity >= similarity_threshold for similarity in similarities)\n",
        "\n",
        "    return EvaluationResult(key=\"must_semantic_match\", score=score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention,semantic_match],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "8b98fff1-bd75-4dbe-cadc-a76d8a82577c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 8eaf11aa' at:\n",
            "https://smith.langchain.com/o/e319c8f1-73bd-5d44-8897-d19a191ebc54/datasets/8c9adff8-853f-46bd-82ac-c48e51ccd15a/compare?selectedSessions=717135f8-68a1-491a-9f2c-b966f4da9e30\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - f27ed691 at:\n",
            "https://smith.langchain.com/o/e319c8f1-73bd-5d44-8897-d19a191ebc54/datasets/8c9adff8-853f-46bd-82ac-c48e51ccd15a\n",
            "[------------------------------------------------->] 15/15"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>feedback.must_semantic_match</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>0</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1f2a3d4b-34d1-4c6b-89eb-923db57eb674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.265765</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.606863</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.722211</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.653603</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6.200908</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.288406</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>12.357677</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       feedback.must_mention feedback.must_semantic_match error  \\\n",
              "count                     15                           15     0   \n",
              "unique                     1                            1     0   \n",
              "top                     True                         True   NaN   \n",
              "freq                      15                           15   NaN   \n",
              "mean                     NaN                          NaN   NaN   \n",
              "std                      NaN                          NaN   NaN   \n",
              "min                      NaN                          NaN   NaN   \n",
              "25%                      NaN                          NaN   NaN   \n",
              "50%                      NaN                          NaN   NaN   \n",
              "75%                      NaN                          NaN   NaN   \n",
              "max                      NaN                          NaN   NaN   \n",
              "\n",
              "        execution_time                                run_id  \n",
              "count        15.000000                                    15  \n",
              "unique             NaN                                    15  \n",
              "top                NaN  1f2a3d4b-34d1-4c6b-89eb-923db57eb674  \n",
              "freq               NaN                                     1  \n",
              "mean          6.265765                                   NaN  \n",
              "std           2.606863                                   NaN  \n",
              "min           1.722211                                   NaN  \n",
              "25%           4.653603                                   NaN  \n",
              "50%           6.200908                                   NaN  \n",
              "75%           7.288406                                   NaN  \n",
              "max          12.357677                                   NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 8eaf11aa',\n",
              " 'results': {'abe28e08-a833-48ed-8712-0f9aed69f84b': {'input': {'question': 'What limitations of QLoRA are discussed in the paper?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('1bd8e610-5558-4714-bc0e-189be13f44b3'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('bec6004a-fd3f-42f6-b422-047ab0b57242'), target_run_id=None)],\n",
              "   'execution_time': 6.728893,\n",
              "   'run_id': '1f2a3d4b-34d1-4c6b-89eb-923db57eb674',\n",
              "   'output': 'The paper \"QLoRA: Efficient Finetuning of Quantized LLMs\" discusses several limitations of QLoRA:\\n\\n1. **Memory Management**: While QLoRA introduces innovations like 4-bit NormalFloat (NF4) and double quantization to reduce memory usage, managing memory spikes remains a challenge. The use of paged optimizers helps, but it is not a complete solution.\\n\\n2. **Evaluation Benchmarks**: The paper highlights that current chatbot benchmarks are not trustworthy for accurately evaluating the performance levels of chatbots. This makes it difficult to assess the true effectiveness of QLoRA in some contexts.\\n\\n3. **Performance Gaps**: Despite achieving high performance, QLoRA does not completely match the performance of models like ChatGPT. The paper includes a \"lemon-picked\" analysis to demonstrate specific scenarios where QLoRA (Guanaco) fails compared to ChatGPT.\\n\\n4. **Resource Constraints**: Although QLoRA allows for finetuning large models on a single GPU, it still requires a high-quality dataset and significant computational resources for optimal performance.\\n\\n5. **Generalization**: The paper suggests that while QLoRA performs well on specific tasks and datasets, its generalization capabilities across diverse tasks and datasets need further exploration.\\n\\nThese limitations indicate areas where QLoRA can be improved and where further research is needed to enhance its efficiency and effectiveness.',\n",
              "   'reference': {'answer': {'must_mention': ['limitations', 'future work']}}},\n",
              "  'b0d55ea7-e424-4b04-b537-f2aa8fe8f3f6': {'input': {'question': 'How does QLoRA differ from other fine-tuning methods?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a59acffb-2f24-4d10-89b2-6e2ce2d95872'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a5c00256-a8b9-425c-b6a4-315c2ef39136'), target_run_id=None)],\n",
              "   'execution_time': 9.508985,\n",
              "   'run_id': 'd2b785d9-7a75-4d3a-8783-f0d1c5a90975',\n",
              "   'output': \"QLoRA (Quantized Low Rank Adapter) is a fine-tuning method that differs from other fine-tuning techniques in several key ways:\\n\\n1. **Quantization Focus**: QLoRA specifically targets the quantization of large language models (LLMs) to make them more efficient for deployment on resource-constrained hardware. This involves reducing the bit-width of the model parameters while retaining as much of the original information as possible.\\n\\n2. **Information Retention**: The IR-QLoRA variant introduces two main technologies to enhance information retention:\\n   - **Statistics-based Information Calibration Quantization**: This technique ensures that the quantized parameters of the LLM retain the original information accurately.\\n   - **Finetuning-based Information Elastic Connection**: This allows LoRA to utilize elastic representation transformation with diverse information, improving the model's ability to adapt during fine-tuning.\\n\\n3. **Performance and Efficiency**: IR-QLoRA has been shown to significantly improve the accuracy of quantized LLMs, such as the LLaMA and LLaMA2 families, with minimal additional time consumption. For example, a 4-bit LLaMA-7B model achieved a 1.4% improvement on the MMLU benchmark compared to state-of-the-art methods, with only a 0.31% increase in time consumption.\\n\\n4. **Versatility**: IR-QLoRA is compatible with various quantization frameworks (e.g., NormalFloat and Integer quantization) and provides general accuracy gains across different models and tasks.\\n\\n5. **Comparison with Other Methods**: In a comparative study, QLoRA was evaluated alongside other fine-tuning methods such as Retrieval Augmented Fine-Tuning (RAFT) and Reinforcement Learning from Human Feedback (RLHF). While RAFT outperformed QLoRA in some cases, QLoRA still demonstrated significant improvements and efficiency, making it a viable option for specific use cases.\\n\\nIn summary, QLoRA stands out due to its focus on efficient quantization and information retention, making it particularly suitable for deploying LLMs on hardware with limited resources.\",\n",
              "   'reference': {'answer': {'must_mention': ['contrast',\n",
              "      'methods',\n",
              "      'fine-tuning']}}},\n",
              "  '99a9d8f1-08d5-44e2-b71c-489877e308e9': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('2a00c081-96b4-46c8-9b85-d29deed05662'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6df4ee39-26db-48e7-8fba-ce4d2372af94'), target_run_id=None)],\n",
              "   'execution_time': 4.174045,\n",
              "   'run_id': '2859258d-5af5-4f85-aba3-8d4f39192e1c',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) uses a combination of optimizers to manage memory efficiently during the fine-tuning of large language models. Specifically, it introduces \"paged optimizers\" to handle memory spikes. This approach allows QLoRA to backpropagate gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA), significantly reducing memory usage without sacrificing performance.',\n",
              "   'reference': {'answer': {'must_mention': ['paged', 'optimizer']}}},\n",
              "  '6454a58e-36a5-4a67-a1ee-833493fcb66e': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('23aade4e-ee75-448f-bcca-1046fba61fcf'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a2e8cce4-b502-45a2-8951-30daae467f90'), target_run_id=None)],\n",
              "   'execution_time': 3.637737,\n",
              "   'run_id': '130bef6a-fc21-4cb6-a83d-55da52f97bb8',\n",
              "   'output': 'In the QLoRA paper, a new data type called **4-bit NormalFloat (NF4)** was introduced. This data type is designed to be information-theoretically optimal for normally distributed weights, which helps in reducing memory usage while preserving performance during the finetuning of quantized large language models (LLMs).',\n",
              "   'reference': {'answer': {'must_mention': ['NF4', 'NormalFloat']}}},\n",
              "  '754a52a2-8096-4f65-979e-c6958bd7a0e7': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('41d4ab5a-153c-45d4-8cb4-f12967b9aa21'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('159021ee-662b-4bd8-b617-5aad3a532e8d'), target_run_id=None)],\n",
              "   'execution_time': 7.274789,\n",
              "   'run_id': '8e841903-abe1-4ced-ba6c-4fefa237c05f',\n",
              "   'output': \"A Retrieval Augmented Generation (RAG) system is a type of artificial intelligence model that combines retrieval-based and generation-based approaches to improve the quality and relevance of generated responses. Here's a breakdown of the two components:\\n\\n1. **Retrieval-Based Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus or database based on the input query. It uses techniques from information retrieval to find the most relevant content that can help answer the query.\\n\\n2. **Generation-Based Component**: This part of the system uses a generative model, such as a transformer-based language model (e.g., GPT-3), to generate a coherent and contextually appropriate response. The generative model can use the retrieved documents as additional context to produce more accurate and informative answers.\\n\\n### How RAG Works\\n1. **Query Input**: The system receives a query or question from the user.\\n2. **Document Retrieval**: The retrieval component searches a large corpus to find documents or passages that are relevant to the query.\\n3. **Contextual Generation**: The generative model takes the original query and the retrieved documents as input to generate a response. The additional context from the retrieved documents helps the model produce more accurate and contextually relevant answers.\\n\\n### Benefits of RAG\\n- **Improved Accuracy**: By leveraging relevant documents, the system can provide more accurate and detailed responses.\\n- **Contextual Relevance**: The retrieved documents provide additional context, making the generated responses more relevant to the user's query.\\n- **Scalability**: The system can handle a wide range of queries by retrieving information from a large and diverse corpus.\\n\\n### Applications\\n- **Question Answering Systems**: Providing detailed and accurate answers to user queries.\\n- **Customer Support**: Assisting customer service agents by retrieving relevant information and generating helpful responses.\\n- **Content Creation**: Aiding in the creation of articles, reports, and other content by providing relevant information and context.\\n\\nOverall, a Retrieval Augmented Generation system enhances the capabilities of generative models by incorporating relevant external information, leading to more accurate and contextually appropriate responses.\",\n",
              "   'reference': {'answer': {'must_mention': ['ground', 'context']}}},\n",
              "  '263c765f-1016-4f8b-9d00-6dae7c283285': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('d6825a04-7a9e-4e19-8fcf-504523569446'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('cb0181db-f6bd-48bb-8f69-0491680bd142'), target_run_id=None)],\n",
              "   'execution_time': 3.797446,\n",
              "   'run_id': 'f0eb90b4-41b8-4d31-8030-98fafe135148',\n",
              "   'output': 'The QLoRA paper titled \"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\" was authored by:\\n\\n- Haotong Qin\\n- Xudong Ma\\n- Xingyu Zheng\\n- Xiaoyang Li\\n- Yang Zhang\\n- Shouda Liu\\n- Jie Luo\\n- Xianglong Liu\\n- Michele Magno\\n\\nYou can find more details and the paper itself [here](https://github.com/htqin/ir-qlora).',\n",
              "   'reference': {'answer': {'must_mention': ['Tim', 'Dettmers']}}},\n",
              "  '5e98fdb0-0478-4c25-b40a-f037e890de1c': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('499e967a-9512-435c-b301-466cdd4b609a'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('df2023ce-b346-4e63-92a7-50052d2e5f85'), target_run_id=None)],\n",
              "   'execution_time': 5.133161,\n",
              "   'run_id': '8330c5aa-3224-4fcc-9b5c-d12f0f1f6c3d',\n",
              "   'output': 'The most popular deep learning frameworks as of 2023 are:\\n\\n1. **TensorFlow**: Developed by Google, TensorFlow is widely used for numerical computation and deep learning. It has a strong community and is used by major corporations like Airbnb and Intel.\\n\\n2. **PyTorch**: Known for its ease of use and flexibility, PyTorch is the preferred framework for research and state-of-the-art models. It offers excellent GPU capabilities, scalability, and debugging tools.\\n\\n3. **Keras**: Initially developed by Francois Chollet, Keras is a high-level neural network API that runs on top of TensorFlow, Theano, and CNTK. It is known for its simplicity and ease of use.\\n\\nThese frameworks are popular due to their robust features, active community support, and widespread adoption in both industry and academia.',\n",
              "   'reference': {'answer': {'must_mention': ['PyTorch', 'TensorFlow']}}},\n",
              "  '11cd4183-91a1-4a16-b07e-3afc7ac12ed8': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('cf097d68-3686-4c38-a003-d1f454881d79'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('938c545d-20b2-4b74-8f29-0748f65e8a0a'), target_run_id=None)],\n",
              "   'execution_time': 6.200908,\n",
              "   'run_id': '8b7124ac-223e-4790-86fa-2784ed4c03be',\n",
              "   'output': \"The Low-Rank Adaptation (LoRA) system introduces several significant improvements, particularly in the context of fine-tuning large language models (LLMs) and other AI models. Here are some key improvements:\\n\\n1. **Parameter Efficiency**: LoRA modifies only a small, low-rank subset of the model's parameters, making the fine-tuning process more efficient. This reduces the computational resources and time required for training.\\n\\n2. **Performance Enhancement**: LoRA has been shown to unlock the performance of higher-rank models, achieving better results compared to traditional fine-tuning methods. For instance, training with LoRA rank 16 and rank 256 shows significant performance improvements.\\n\\n3. **Cost-Effectiveness**: By focusing on a low-rank subset of parameters, LoRA reduces the overall cost of fine-tuning large models. This makes it a more accessible option for researchers and developers with limited resources.\\n\\n4. **Versatility**: LoRA is applicable to various types of AI models, including large language models, diffusion models for image generation, and vision transformers. This versatility makes it a valuable tool across different domains in AI.\\n\\n5. **Scalability**: The method allows for scalable fine-tuning, meaning it can be effectively applied to models of different sizes and complexities without a significant increase in computational burden.\\n\\nOverall, LoRA provides a more efficient, cost-effective, and versatile approach to fine-tuning AI models, making it a popular choice in the AI research community.\",\n",
              "   'reference': {'answer': {'must_mention': ['reduce', 'parameters']}}},\n",
              "  '49a903b0-e813-4676-bd1f-cd10927fe69c': {'input': {'question': 'What is the primary goal of the QLoRA method?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('351b0649-9c82-4433-a2af-c7e3ce43d82d'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('6210aa9d-c102-455a-a43c-75c0b0524857'), target_run_id=None)],\n",
              "   'execution_time': 1.722211,\n",
              "   'run_id': '61c1670c-d33b-4a5d-a365-a77c14fdf640',\n",
              "   'output': \"The primary goal of the QLoRA (Quantized Low-Rank Adaptation) method is to efficiently fine-tune large language models by reducing the computational and memory requirements. This is achieved by quantizing the model parameters to lower precision and applying low-rank adaptations, which allows for faster training and inference while maintaining or even improving the model's performance. This method is particularly useful for adapting pre-trained models to specific tasks or domains without the need for extensive computational resources.\",\n",
              "   'reference': {'answer': {'must_mention': ['efficient', 'fine-tuning']}}},\n",
              "  '49c0ae51-55f6-445f-8a2b-9f7b0967ead6': {'input': {'question': 'What are the main components of the QLoRA approach?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('520586f2-7b6a-47c1-967a-c11071f56cbf'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('40ea372f-b74f-4819-9a9c-82248ef8ceff'), target_run_id=None)],\n",
              "   'execution_time': 5.852832,\n",
              "   'run_id': '57b8b076-d08c-4c7e-829e-f9344a410d62',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) is a technique used to fine-tune large language models efficiently. The main components of the QLoRA approach typically include:\\n\\n1. **Quantization**: This involves reducing the precision of the model weights to lower bit-widths (e.g., from 32-bit floating point to 8-bit integers). Quantization helps in reducing the memory footprint and computational requirements of the model.\\n\\n2. **Low-Rank Adaptation (LoRA)**: LoRA is a method that adapts the pre-trained model by adding low-rank matrices to the original weights. This allows for efficient fine-tuning with fewer parameters, making the process faster and less resource-intensive.\\n\\n3. **Fine-Tuning**: The process of adjusting the model parameters on a specific task or dataset to improve performance. In the context of QLoRA, fine-tuning is done on the quantized and low-rank adapted model.\\n\\n4. **Optimization Algorithms**: Specific optimization algorithms are used to update the model parameters during fine-tuning. These algorithms are designed to work efficiently with quantized and low-rank adapted models.\\n\\n5. **Evaluation Metrics**: Metrics to evaluate the performance of the fine-tuned model on the target task. These metrics help in assessing the effectiveness of the QLoRA approach.\\n\\n6. **Hardware Acceleration**: Utilizing specialized hardware (e.g., GPUs, TPUs) that supports efficient computation with quantized models and low-rank matrices.\\n\\nThese components work together to enable efficient fine-tuning of large language models, making it feasible to adapt them to specific tasks with reduced computational resources.',\n",
              "   'reference': {'answer': {'must_mention': ['low-rank',\n",
              "      'adapters',\n",
              "      'quantization']}}},\n",
              "  'eabc7efb-fd16-4f1d-ab54-0651a548a452': {'input': {'question': 'What datasets were used to evaluate the QLoRA method?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('4e01be23-acf0-4aec-a5ba-068a0743bf4f'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('4c783ad8-489f-4437-a8b6-caf411009afb'), target_run_id=None)],\n",
              "   'execution_time': 12.357677,\n",
              "   'run_id': 'f15caff7-c9de-477c-a9a4-584758d52029',\n",
              "   'output': \"The QLoRA method was evaluated using a variety of datasets and benchmarks. Here are the key datasets and evaluation metrics used:\\n\\n1. **Instruction Datasets**: QLoRA was used to fine-tune more than 1,000 models across 8 instruction datasets. These datasets were used to evaluate instruction-following and chatbot performance.\\n\\n2. **Vicuna Benchmark**: The performance of the QLoRA method was evaluated on the Vicuna benchmark, where it reached 99.3% of the performance level of ChatGPT.\\n\\n3. **Travel Chatbot Dataset**: In a specific use case, a travel dataset sourced from the Reddit API was used. This dataset included travel-related conversation prompts and personalized travel experiences. The dataset was augmented for each fine-tuning method, including QLoRA.\\n\\n4. **Pretrained Models**: The evaluation involved fine-tuning pretrained models such as LLaMa 2 7B and Mistral 7B.\\n\\n5. **Evaluation Metrics**: The performance was evaluated using various metrics, including:\\n   - Traditional NLP metrics\\n   - RAG Assessment (Ragas)\\n   - OpenAI GPT-4 evaluation metrics\\n   - Human evaluation\\n\\n6. **Human and GPT-4 Evaluations**: Both human and GPT-4 evaluations were used to assess chatbot performance, with findings indicating that GPT-4 evaluations are a reasonable alternative to human evaluation.\\n\\nThese datasets and evaluation metrics provided a comprehensive assessment of the QLoRA method's performance across different scenarios and model types.\",\n",
              "   'reference': {'answer': {'must_mention': ['datasets', 'evaluation']}}},\n",
              "  '04484fa3-615e-4f63-94a3-af5dd91980a9': {'input': {'question': 'Who are the primary authors of the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('4e5dab17-77c8-4acc-b5e0-a766936eb919'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('3eb35a94-898a-41de-9336-e757340ae63e'), target_run_id=None)],\n",
              "   'execution_time': 5.317756,\n",
              "   'run_id': 'fc495514-5b7c-4ba2-9846-d87eab64ccaf',\n",
              "   'output': 'The primary authors of the QLoRA paper titled \"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\" are:\\n\\n1. Haotong Qin\\n2. Xudong Ma\\n3. Xingyu Zheng\\n4. Xiaoyang Li\\n5. Yang Zhang\\n6. Shouda Liu\\n7. Jie Luo\\n8. Xianglong Liu\\n9. Michele Magno',\n",
              "   'reference': {'answer': {'must_mention': ['Tim', 'Dettmers']}}},\n",
              "  'f1ab4603-bda8-4963-8974-a02459db65c7': {'input': {'question': 'What models are fine-tuned using the QLoRA method?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('583090b7-658e-44f9-bf2b-9b69bd5193ad'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('a8b121ea-ef63-4459-861e-c1adabedb036'), target_run_id=None)],\n",
              "   'execution_time': 6.530674,\n",
              "   'run_id': '468a1bdf-3837-4b05-bd0d-301ed9f9aa9e',\n",
              "   'output': 'The QLoRA (Quantized Low-Rank Adaptation) method has been used to fine-tune various models, including:\\n\\n1. **Guanaco**: This model family outperforms previous openly released models on the Vicuna benchmark, achieving 99.3% of the performance level of ChatGPT. It was fine-tuned using QLoRA on a single GPU.\\n\\n2. **LLaMA**: QLoRA has been used to fine-tune different scales of the LLaMA model, including 33B and 65B parameter models.\\n\\n3. **T5**: The T5 model has also been fine-tuned using QLoRA.\\n\\n4. **Falcon-40b**: This model was fine-tuned using a variant of QLoRA called QDyLoRA, which allows for dynamic low-rank adaptation.\\n\\nThe QLoRA method is particularly efficient in reducing memory usage, enabling the fine-tuning of large models on hardware with limited memory capacity. This method has been shown to achieve state-of-the-art results even with smaller models when fine-tuned on high-quality datasets.',\n",
              "   'reference': {'answer': {'must_mention': ['transformer',\n",
              "      'models',\n",
              "      'GPT-3',\n",
              "      'BERT']}}},\n",
              "  '6bffa9c8-c2b7-4d75-a421-7be909e0f832': {'input': {'question': 'How does QLoRA achieve efficiency in fine-tuning?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('69627cae-c467-48d0-ac6e-d4e52cad9d68'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('fbf3a5fc-a9f7-412e-ba3f-f1152c71dd67'), target_run_id=None)],\n",
              "   'execution_time': 7.302023,\n",
              "   'run_id': '695cd2f1-3cca-4953-b55c-0a0bf5d01c49',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) achieves efficiency in fine-tuning by combining quantization and low-rank adaptation techniques. Here‚Äôs a detailed breakdown of how it works:\\n\\n1. **Quantization**:\\n   - **Reduced Precision**: QLoRA uses quantization to reduce the precision of the model weights from 32-bit floating-point to lower precision formats such as 8-bit or even lower. This significantly reduces the memory footprint and computational requirements.\\n   - **Efficient Storage and Computation**: By storing weights in a lower precision format, the model requires less memory, which allows for larger models to be fine-tuned on hardware with limited memory. Additionally, lower precision arithmetic operations are faster, leading to reduced training time.\\n\\n2. **Low-Rank Adaptation**:\\n   - **Parameter Efficiency**: Low-rank adaptation involves decomposing the weight matrices into products of smaller matrices. This reduces the number of parameters that need to be updated during fine-tuning, which in turn reduces the computational cost.\\n   - **Faster Convergence**: By focusing on the most significant components of the weight matrices, low-rank adaptation can lead to faster convergence during fine-tuning, as the model can more quickly adapt to the new task with fewer parameters to optimize.\\n\\n3. **Combined Approach**:\\n   - **Memory and Computation Trade-offs**: By combining quantization and low-rank adaptation, QLoRA leverages the strengths of both techniques. Quantization reduces the memory and computational requirements, while low-rank adaptation ensures that the model can still effectively learn and adapt to new tasks with fewer parameters.\\n   - **Scalability**: This combined approach allows for the fine-tuning of very large models on hardware with limited resources, making it more accessible and scalable.\\n\\nOverall, QLoRA achieves efficiency in fine-tuning by reducing the memory and computational requirements through quantization and by optimizing the learning process with low-rank adaptation. This makes it possible to fine-tune large models more quickly and on less powerful hardware.',\n",
              "   'reference': {'answer': {'must_mention': ['quantization', 'efficiency']}}},\n",
              "  '740567df-54b4-4594-8846-20f171c12444': {'input': {'question': 'What are the key results reported in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('04e89a62-c3cf-449d-a415-6f0dc095e175'), target_run_id=None),\n",
              "    EvaluationResult(key='must_semantic_match', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('2d5d16e7-8eec-4a7a-90b0-9495f34bb8f8'), target_run_id=None)],\n",
              "   'execution_time': 8.447339,\n",
              "   'run_id': '27863086-271f-4820-8c82-0cd4a3f10c92',\n",
              "   'output': 'The key results reported in the QLoRA paper, specifically from the work titled \"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\" by Haotong Qin et al., are as follows:\\n\\n1. **Introduction of IR-QLoRA**: The paper proposes a novel method called IR-QLoRA (Information Retention Quantized Low-Rank Adaptation) aimed at improving the accuracy of quantized large language models (LLMs) that use LoRA (Low-Rank Adaptation).\\n\\n2. **Technological Innovations**:\\n   - **Statistics-based Information Calibration Quantization**: This technique ensures that the quantized parameters of LLMs retain their original information accurately.\\n   - **Finetuning-based Information Elastic Connection**: This allows LoRA to utilize elastic representation transformation with diverse information, enhancing the model\\'s adaptability and performance.\\n\\n3. **Performance Improvements**:\\n   - The IR-QLoRA method significantly improves accuracy across LLaMA and LLaMA2 families under 2-4 bit-widths.\\n   - For instance, a 4-bit LLaMA-7B model achieves a 1.4% improvement on the MMLU benchmark compared to state-of-the-art methods.\\n\\n4. **Efficiency**:\\n   - The performance gains from IR-QLoRA require only a minimal additional time consumption of 0.31%, indicating high efficiency.\\n\\n5. **Versatility**:\\n   - IR-QLoRA is compatible with various quantization frameworks (e.g., NormalFloat and Integer quantization) and provides general accuracy gains across different setups.\\n\\n6. **Availability**:\\n   - The code for IR-QLoRA is made available to the public, promoting further research and application in the field.\\n\\nThese results highlight the effectiveness of IR-QLoRA in enhancing the performance and efficiency of quantized LLMs, making them more suitable for deployment on resource-constrained hardware.',\n",
              "   'reference': {'answer': {'must_mention': ['results',\n",
              "      'benchmarks',\n",
              "      'performance']}}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Semantic Match Langsmith \n",
        "\n",
        "![Langsmith](./semantic-match.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "We are instantiating a new `StateGraph` as `graph_with_helpfulness_check` which uses the `AgentState` to keep track of the current state when it is passed to another agent or a nodes.The idea is to run through the graph and finally determine the helpfulness of the response generated by the underlying LLMs and the tools.\n",
        "Then we are adding two nodes in the graph where the first node is `agent` and set to call the fuction `call_model` and the second node is `action` which is added as `tool_node`. \n",
        "Just to remind what the `call_model` and `tool_node` represents here\n",
        "   `call_model` - Is a custom fuction which takes the current state and calls the LLM model then the response is sent back to the caller.\n",
        "   `tool_node` - Is an object of class `ToolNode` and instantiated with a list of tools to consume. This tool node is capable of using the tools `duckduckgo_search`, `arxiv_query`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "Now we are goint to set an entry point which is in our case an `agent` This agent is responsible for understanding the user questions and confer with LLM to see if we need to use any tools subsequently to generate a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "This function, tool_call_or_helpful, is designed to decide whether to invoke an action node or assess the helpfulness of a response generated by a StateGraph.\n",
        "\n",
        "The function first examines the latest message in the state to determine if it includes any tool calls using the `AIMessage.tool_calls` property. If tool calls are present, it immediately returns \"action\", indicating that an action node should be called.\n",
        "\n",
        "If no tool calls are detected, the function proceeds to evaluate the helpfulness of the response. It does this using initial query and the final response from the state message history. \n",
        "\n",
        "If the number of messages in the conversation exceeds 10, the function returns \"END\", terminating the process. This is to avoid the graph going in loop until helpful response is generated. \n",
        "\n",
        "A prompt template is created, instructing the model to assess the helpfulness of the final response based on the initial query. The template expects a 'Y' for helpful and an 'N' for unhelpful.\n",
        "This prompt template is then used to construct a chain, which includes the ChatOpenAI model (using GPT-4) and a StrOutputParser to interpret the model‚Äôs output.\n",
        "\n",
        "The model is invoked with the initial query and final response, and the output is parsed to determine if the response is helpful ('Y') or not ('N').\n",
        "\n",
        "If the response is found to be helpful ('Y'), the function returns \"end\", indicating that the process should stop. Otherwise, it returns \"continue\", suggesting the need for further iteration in the loop until a helpful response is generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "1. Retrieve the last message from the state to check if it contains any tool calls.\n",
        "\n",
        "2. Use the tool_calls attribute of the last message to determine if an action node needs to be invoked. If tool calls are found, return \"action\".\n",
        "\n",
        "3. Extract the first and last messages in the state for comparison to determine the helpfulness of the response.\n",
        "\n",
        "4. Check if the total number of messages in the state exceeds 10. If it does, return \"END\" to terminate further processing.\n",
        "\n",
        "5. Create a prompt template that instructs the language model to evaluate the helpfulness of the response based on the initial query and final response.\n",
        "\n",
        "6. Construct a processing chain that includes the language model (ChatOpenAI) and an output parser (StrOutputParser) to handle the model's output.\n",
        "\n",
        "7. Use the defined prompt template and chain to invoke the model, passing in the initial query and final response for evaluation.\n",
        "\n",
        "8. Parse the model‚Äôs output to check if the response was deemed helpful ('Y') or not ('N').\n",
        "\n",
        "9. If the response is helpful, return \"end\". If not, return \"continue\" to indicate that further iterations are required to find a helpful response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now add conditional edges for the node `agent` where a decision is made by calling the function `tool_call_or_helpful` and the output is lookedup in the objects `path_map` (3rd arguments in the `add_conditional_edges` function). \n",
        "If the function return `continue` that means it will start another iteration of the same task and it indicates that the generated content is not helpfull and failed by our helpfulness check.\n",
        "If the function return `action` that means a tool call is necessary to have relevant information for generating valid and quality output. \n",
        "If the function return `end` then we reached the maximum number of iteration that we are allowed to perform. This limit is imposed to avoid the graph going into infinite loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "Add the final communication link from `action` node to `agent` node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "Once the the nodes, edges are setup in the graph, we compile the graph ready for invokation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "Below code invokes the agent asynchronously to process a query and stream updates as the agent progresses through its processing graph. The agent processes the input query in real-time, providing incremental updates from different nodes, and these updates are printed out as they are received.\n",
        "\n",
        "The human message contains 3 questions in the same prompt. As we can see from the output below that the output of the agent is empty but the `tool_calls` property has 3 entries. The LLM model understood that there are 3 seperate questions to seach in the internet and rightly it picked the `duckduckgo_search` tools for searching the internet.\n",
        "\n",
        "As per our conditional edge setup the next action would be to invoke he `action` node which runs necessary tools. Once all actions are complete the responses are sent back to the `agent'.\n",
        "\n",
        "Finally the agent is generates a response based on the searches maded previously. Then the execution stopped as the agent ouput is checked for helpfulness. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "22470df4-9aa7-4751-95b6-d30ce71b17db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_57FNgm5EPmrnYbi6sOtly32Q', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_LYZodyniCY5ww90bFvIttzZU', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_i7snUV6tzNiPAA5POOrZqvPJ', 'function': {'arguments': '{\"query\": \"Attention in machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 171, 'total_tokens': 247}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-67d9d37a-5b38-4c1b-8049-95ad8336e99e-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_57FNgm5EPmrnYbi6sOtly32Q', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_LYZodyniCY5ww90bFvIttzZU', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Attention in machine learning'}, 'id': 'call_i7snUV6tzNiPAA5POOrZqvPJ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 76, 'total_tokens': 247})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='Let\\'s jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights LoRA\\'s approach to decomposing ( Œî W ) into a product of lower rank matrices effectively balances the need to adapt large pre-trained models to new tasks while maintaining computational efficiency. The intrinsic rank concept is key to this balance, ensuring that the essence of the model\\'s learning capability is preserved with significantly ... Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning ($\\\\\\\\approx$100K prompt-response ... \"Lora The Tuner\" By Daniel Warfield using MidJourney. All images by the author unless otherwise specified. Fine tuning is the process of tailoring a machine learning model to a specific application, which can be vital in achieving consistent and high quality performance. Feb 18, 2024. Share. Ever since the introduction of BERT in 2019, fine-tuning has been the standard approach to adapt large language models (LLMs) to downstream tasks. This changed with the introduction of LoRA (Hu et al 2021) which showed for the first time that the weight update matrix during fine-tuning can be drastically simplified using ...', name='duckduckgo_search', tool_call_id='call_57FNgm5EPmrnYbi6sOtly32Q'), ToolMessage(content='Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... ‚Äî Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization ... Tim Dettmers. Video. Tech Moves: AI researcher Yejin Choi leaves Univ. of Washington and Allen Institute for AI. by Todd Bishop & Taylor Soper on August 2, 2024 August 2, 2024 at 11:59 am. Its purpose is to make cutting-edge research by Tim Dettmers, a leading academic expert on quantization and the use of deep learning hardware accelerators, accessible to the general public. QLoRA: One of the core contributions of bitsandbytes towards the democratization of AI. If you have a curiosity about how fancy graphics cards actually work, and why they are so well-suited to AI-type applications, then take a few minutes to read [Tim Dettmers] explain why this is so.‚Ä¶', name='duckduckgo_search', tool_call_id='call_LYZodyniCY5ww90bFvIttzZU'), ToolMessage(content='Learn how attention mechanisms in deep learning enable models to focus on relevant information and improve performance in tasks such as machine translation, image captioning, and speech recognition. Understand the steps and components of attention mechanism architecture and see examples of its applications. Attention mechanism is a fundamental invention in artificial intelligence and machine learning, redefining the capabilities of deep learning models. This mechanism, inspired by the human mental process of selective focus, has emerged as a pillar in a variety of applications, accelerating developments in natural language processing, computer vision, and beyond. They all use transformer architecture with attention mechanisms at their core to solve problems across domains. In the Transformer series, we go over the ingredients that have made Transformers a universal recipe for machine learning. First up, we take a visual dive to understand the attention mechanism: Why transformers and attention took over. This example starts with an imaginary sequence of four words (e.g. forming a sentence that needs to be translated): \"<word 1> <word 2> <word 3> <word 4>\" . Each of the words can be represented ... The concept of \"attention\" in deep learning has its roots in the effort to improve Recurrent Neural Networks (RNNs) for handling longer sequences or sentences. For instance, consider translating a sentence from one language to another. Translating a sentence word-by-word is usually not an option because it ignores the complex grammatical ...', name='duckduckgo_search', tool_call_id='call_i7snUV6tzNiPAA5POOrZqvPJ')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"### LoRA in Machine Learning\\nLoRA, or Low-Rank Adaptation, is a parameter-efficient fine-tuning method for large language models (LLMs). Traditional fine-tuning involves adjusting the pre-trained weights of a model to adapt it to new tasks, which can be computationally expensive. LoRA simplifies this process by decomposing the weight update matrix into a product of lower-rank matrices. This approach significantly reduces the memory and computational requirements while maintaining the model's performance. LoRA is particularly useful for adapting large pre-trained models to new tasks without the need for extensive computational resources.\\n\\n### Tim Dettmers\\nTim Dettmers is a researcher specializing in efficient deep learning, particularly at the intersection of machine learning, natural language processing (NLP), and computer systems. He is known for his work on quantization and the use of deep learning hardware accelerators. One of his notable contributions is QLoRA, a method that drastically reduces the GPU memory required to fine-tune large models, making it possible to fine-tune a 65 billion parameter model on a single GPU. Tim Dettmers is affiliated with the Allen Institute for AI and has made significant contributions to the democratization of AI through his research.\\n\\n### Attention in Machine Learning\\nThe attention mechanism is a fundamental concept in deep learning that allows models to focus on relevant parts of the input data, improving performance in various tasks such as machine translation, image captioning, and speech recognition. Inspired by the human cognitive process of selective focus, attention mechanisms enable models to weigh the importance of different input elements dynamically. This mechanism is a core component of transformer architectures, which have revolutionized natural language processing and other fields. Attention mechanisms help models handle longer sequences and capture complex dependencies, making them more effective and efficient in processing and generating data.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 364, 'prompt_tokens': 1166, 'total_tokens': 1530}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-0bf05216-2138-4ec5-960e-5d09a678076d-0', usage_metadata={'input_tokens': 1166, 'output_tokens': 364, 'total_tokens': 1530})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "0593c6f5-0a1d-41f6-960b-f32d101b3ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt engineering is a concept primarily associated with the field of artificial intelligence, particularly in the context of natural language processing (NLP) and large language models like GPT-3. It involves the design and crafting of prompts (input text) to elicit desired responses from AI models. The goal is to optimize the input to get the most accurate, relevant, or useful output from the model.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "1. **Crafting Effective Prompts**: Designing prompts that are clear, specific, and structured in a way that the AI can understand and respond to appropriately.\n",
            "2. **Iterative Testing**: Continuously refining prompts based on the responses received to improve the quality and relevance of the output.\n",
            "3. **Understanding Model Behavior**: Gaining insights into how the model interprets different types of input to better predict and guide its responses.\n",
            "\n",
            "### Emergence of Prompt Engineering:\n",
            "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The ability of these models to generate human-like text based on prompts led to a growing interest in how to effectively communicate with them to achieve specific goals. The term \"prompt engineering\" itself started gaining traction around this time as researchers and practitioners began to explore and document best practices for interacting with these advanced AI systems.\n",
            "\n",
            "To get more precise information on the timeline and development of prompt engineering, I can look up recent articles or papers on the topic. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n",
            "RAG stands for Retrieval-Augmented Generation. It is a technique in natural language processing (NLP) that combines retrieval-based methods with generative models to improve the quality and relevance of generated text. The key idea is to retrieve relevant documents or pieces of information from a large corpus and use this retrieved information to guide the generation process.\n",
            "\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The approach leverages both the strengths of retrieval-based models, which can access a vast amount of information, and generative models, which can produce coherent and contextually appropriate text.\n",
            "\n",
            "Would you like more detailed information or specific aspects of RAG?\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large dataset, making it more efficient and effective for specialized tasks. Fine-tuning is particularly popular in natural language processing (NLP) and computer vision.\n",
            "\n",
            "### Key Steps in Fine-Tuning:\n",
            "1. **Pre-training**: A model is initially trained on a large, general dataset.\n",
            "2. **Fine-tuning**: The pre-trained model is then trained on a smaller, task-specific dataset.\n",
            "\n",
            "### Benefits of Fine-Tuning:\n",
            "- **Efficiency**: Requires less data and computational resources compared to training a model from scratch.\n",
            "- **Performance**: Often results in better performance on the specific task due to the model's prior knowledge.\n",
            "\n",
            "### Historical Context:\n",
            "Fine-tuning became particularly prominent with the advent of large pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer). These models demonstrated that fine-tuning could achieve state-of-the-art results on a variety of NLP tasks.\n",
            "\n",
            "To pinpoint the exact timeline and key papers that popularized fine-tuning, I can look up relevant scientific articles and historical milestones. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n",
            "### What are LLM-based Agents?\n",
            "\n",
            "LLM-based agents are systems that leverage Large Language Models (LLMs) to perform tasks autonomously or semi-autonomously. These agents can interact with their environment, process natural language, and make decisions based on the information they perceive. They are often used in various applications, including software engineering, customer service, and more.\n",
            "\n",
            "### History and Emergence\n",
            "\n",
            "The concept of LLM-based agents has evolved alongside advancements in artificial intelligence and natural language processing. Here are some key points in their development:\n",
            "\n",
            "1. **Early Research and Development**: The idea of socially interactive agents (SIAs) has been around for over 20 years, with significant research and technological advancements enabling their use in everyday interfaces and environments. This includes both virtual and physical agents.\n",
            "\n",
            "2. **Rise of Large Language Models**: The development of large language models like GPT-3 by OpenAI marked a significant milestone. These models demonstrated the ability to generate human-like text and understand context, which are crucial capabilities for LLM-based agents.\n",
            "\n",
            "3. **Applications in Various Domains**: Researchers have been exploring the applications of LLMs in various domains, such as code generation, vulnerability detection, and more. This exploration has highlighted both the potential and limitations of LLM-based agents.\n",
            "\n",
            "4. **Recent Developments**: The field continues to evolve with ongoing research into the behavior, personality, and social phenomena of LLM-based agents. This includes understanding how these agents can interact with humans and other agents in a society.\n",
            "\n",
            "### Key Publications\n",
            "\n",
            "- **\"Appearance\" (2021)**: This paper reviews the history and development of socially interactive agents, focusing on their appearance and the design space for these agents.\n",
            "- **\"Markov Persuasion Processes with Endogenous Agent Beliefs\" (2023)**: This paper explores dynamic Bayesian persuasion settings involving agents, contributing to the understanding of agent beliefs and decision-making processes.\n",
            "- **\"Consensus of Multi-Agent Systems Using Back-Tracking and History Following Algorithms\" (2020)**: This paper proposes algorithms for achieving consensus in multi-agent systems, addressing challenges like communication loss.\n",
            "\n",
            "These publications highlight the ongoing research and development in the field of LLM-based agents, showcasing their potential and the challenges that need to be addressed.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
