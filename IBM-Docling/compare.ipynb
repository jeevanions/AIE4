{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: docling in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (1.12.0)\n",
      "Requirement already satisfied: certifi>=2024.7.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (2024.8.30)\n",
      "Requirement already satisfied: deepsearch-glm<0.22.0,>=0.21.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (0.21.0)\n",
      "Requirement already satisfied: docling-core<2.0.0,>=1.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (1.3.0)\n",
      "Requirement already satisfied: docling-ibm-models<2.0.0,>=1.1.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (1.1.7)\n",
      "Requirement already satisfied: docling-parse<2.0.0,>=1.2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (1.2.0)\n",
      "Requirement already satisfied: easyocr<2.0,>=1.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (1.7.1)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (1.2.0)\n",
      "Requirement already satisfied: huggingface_hub<1,>=0.23 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (0.23.2)\n",
      "Requirement already satisfied: pyarrow<17.0.0,>=16.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (16.1.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (2.8.2)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (2.5.2)\n",
      "Requirement already satisfied: pypdfium2<5.0.0,>=4.30.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (4.30.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (2.32.3)\n",
      "Requirement already satisfied: rtree<2.0.0,>=1.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (1.3.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.14.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (1.14.1)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.2.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (2.4.1)\n",
      "Requirement already satisfied: torchvision<1,>=0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (0.19.1)\n",
      "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling) (0.12.5)\n",
      "Requirement already satisfied: docutils!=0.21 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from deepsearch-glm<0.22.0,>=0.21.0->docling) (0.21.2)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.7.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from deepsearch-glm<0.22.0,>=0.21.0->docling) (3.9.2)\n",
      "Requirement already satisfied: networkx<4.0,>=3.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from deepsearch-glm<0.22.0,>=0.21.0->docling) (3.3)\n",
      "Requirement already satisfied: netwulf<0.2.0,>=0.1.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from deepsearch-glm<0.22.0,>=0.21.0->docling) (0.1.5)\n",
      "Requirement already satisfied: numerize<0.13,>=0.12 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from deepsearch-glm<0.22.0,>=0.21.0->docling) (0.12)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from deepsearch-glm<0.22.0,>=0.21.0->docling) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.5.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from deepsearch-glm<0.22.0,>=0.21.0->docling) (2.2.2)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from deepsearch-glm<0.22.0,>=0.21.0->docling) (1.0.1)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from deepsearch-glm<0.22.0,>=0.21.0->docling) (13.8.1)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from deepsearch-glm<0.22.0,>=0.21.0->docling) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.64.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from deepsearch-glm<0.22.0,>=0.21.0->docling) (4.66.4)\n",
      "Requirement already satisfied: json-schema-for-humans<2.0.0,>=1.0.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-core<2.0.0,>=1.3.0->docling) (1.0.2)\n",
      "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-core<2.0.0,>=1.3.0->docling) (1.1.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-core<2.0.0,>=1.3.0->docling) (4.22.0)\n",
      "Requirement already satisfied: pyproject-toml<0.0.11,>=0.0.10 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-core<2.0.0,>=1.3.0->docling) (0.0.10)\n",
      "Requirement already satisfied: Pillow<11.0.0,>=10.0.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-ibm-models<2.0.0,>=1.1.7->docling) (10.3.0)\n",
      "Requirement already satisfied: jsonlines<4.0.0,>=3.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-ibm-models<2.0.0,>=1.1.7->docling) (3.1.0)\n",
      "Requirement already satisfied: lxml<5.0.0,>=4.9.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-ibm-models<2.0.0,>=1.1.7->docling) (4.9.4)\n",
      "Requirement already satisfied: mean_average_precision<2022.0.0.0,>=2021.4.26.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-ibm-models<2.0.0,>=1.1.7->docling) (2021.4.26.0)\n",
      "Requirement already satisfied: onnxruntime<2.0.0,>=1.16.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-ibm-models<2.0.0,>=1.1.7->docling) (1.19.2)\n",
      "Requirement already satisfied: opencv-python-headless<5.0.0.0,>=4.9.0.80 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-ibm-models<2.0.0,>=1.1.7->docling) (4.10.0.84)\n",
      "Requirement already satisfied: scikit-image in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling) (0.24.0)\n",
      "Requirement already satisfied: python-bidi in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling) (0.6.0)\n",
      "Requirement already satisfied: PyYAML in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling) (6.0.1)\n",
      "Requirement already satisfied: Shapely in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling) (2.0.6)\n",
      "Requirement already satisfied: pyclipper in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling) (1.3.0.post5)\n",
      "Requirement already satisfied: ninja in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from easyocr<2.0,>=1.7->docling) (1.11.1.1)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from huggingface_hub<1,>=0.23->docling) (4.11.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.0.0->docling) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->docling) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->docling) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3.0.0,>=2.32.3->docling) (2.2.1)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling) (1.13.2)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from torch<3.0.0,>=2.2.2->docling) (3.1.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from typer<0.13.0,>=0.12.5->docling) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from typer<0.13.0,>=0.12.5->docling) (1.5.4)\n",
      "Requirement already satisfied: MarkupSafe<3.0,>=2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core<2.0.0,>=1.3.0->docling) (2.1.5)\n",
      "Requirement already satisfied: Pygments<3.0.0,>=2.10.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core<2.0.0,>=1.3.0->docling) (2.18.0)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.6 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core<2.0.0,>=1.3.0->docling) (0.5.14)\n",
      "Requirement already satisfied: htmlmin<0.2.0,>=0.1.12 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core<2.0.0,>=1.3.0->docling) (0.1.12)\n",
      "Requirement already satisfied: markdown2<3.0.0,>=2.4.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core<2.0.0,>=1.3.0->docling) (2.5.0)\n",
      "Requirement already satisfied: pytz in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core<2.0.0,>=1.3.0->docling) (2024.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonlines<4.0.0,>=3.1.0->docling-ibm-models<2.0.0,>=1.1.7->docling) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<2.0.0,>=1.3.0->docling) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<2.0.0,>=1.3.0->docling) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core<2.0.0,>=1.3.0->docling) (0.18.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.7.1->deepsearch-glm<0.22.0,>=0.21.0->docling) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.7.1->deepsearch-glm<0.22.0,>=0.21.0->docling) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.7.1->deepsearch-glm<0.22.0,>=0.21.0->docling) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.7.1->deepsearch-glm<0.22.0,>=0.21.0->docling) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.7.1->deepsearch-glm<0.22.0,>=0.21.0->docling) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from matplotlib<4.0.0,>=3.7.1->deepsearch-glm<0.22.0,>=0.21.0->docling) (2.9.0.post0)\n",
      "Requirement already satisfied: simplejson>=3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from netwulf<0.2.0,>=0.1.5->deepsearch-glm<0.22.0,>=0.21.0->docling) (3.19.3)\n",
      "Requirement already satisfied: coloredlogs in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from onnxruntime<2.0.0,>=1.16.2->docling-ibm-models<2.0.0,>=1.1.7->docling) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from onnxruntime<2.0.0,>=1.16.2->docling-ibm-models<2.0.0,>=1.1.7->docling) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from onnxruntime<2.0.0,>=1.16.2->docling-ibm-models<2.0.0,>=1.1.7->docling) (5.27.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pandas>=1.5.1->deepsearch-glm<0.22.0,>=0.21.0->docling) (2024.1)\n",
      "Requirement already satisfied: setuptools>=42 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pyproject-toml<0.0.11,>=0.0.10->docling-core<2.0.0,>=1.3.0->docling) (69.5.1)\n",
      "Requirement already satisfied: wheel in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pyproject-toml<0.0.11,>=0.0.10->docling-core<2.0.0,>=1.3.0->docling) (0.43.0)\n",
      "Requirement already satisfied: toml in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pyproject-toml<0.0.11,>=0.0.10->docling-core<2.0.0,>=1.3.0->docling) (0.10.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from rich<14.0.0,>=13.7.0->deepsearch-glm<0.22.0,>=0.21.0->docling) (3.0.0)\n",
      "Requirement already satisfied: imageio>=2.33 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2.35.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (2024.8.30)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from scikit-image->easyocr<2.0,>=1.7->docling) (0.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from sympy->torch<3.0.0,>=2.2.2->docling) (1.3.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.6->json-schema-for-humans<2.0.0,>=1.0.0->docling-core<2.0.0,>=1.3.0->docling) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.6->json-schema-for-humans<2.0.0,>=1.0.0->docling-core<2.0.0,>=1.3.0->docling) (0.9.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->deepsearch-glm<0.22.0,>=0.21.0->docling) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.1->deepsearch-glm<0.22.0,>=0.21.0->docling) (1.16.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from coloredlogs->onnxruntime<2.0.0,>=1.16.2->docling-ibm-models<2.0.0,>=1.1.7->docling) (10.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.6->json-schema-for-humans<2.0.0,>=1.0.0->docling-core<2.0.0,>=1.3.0->docling) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: docling-core in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: langchain-text-splitters in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (0.2.4)\n",
      "Requirement already satisfied: pymupdf4llm in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (0.0.14)\n",
      "Requirement already satisfied: pymupdf in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (1.24.10)\n",
      "Requirement already satisfied: ragas in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (0.1.18)\n",
      "Requirement already satisfied: json-schema-for-humans<2.0.0,>=1.0.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-core) (1.0.2)\n",
      "Requirement already satisfied: jsonref<2.0.0,>=1.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-core) (1.1.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-core) (4.22.0)\n",
      "Requirement already satisfied: pandas<3.0.0,>=2.2.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-core) (2.2.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.6.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-core) (2.8.2)\n",
      "Requirement already satisfied: pyproject-toml<0.0.11,>=0.0.10 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-core) (0.0.10)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from docling-core) (0.9.0)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-text-splitters) (0.2.40)\n",
      "Requirement already satisfied: PyMuPDFb==1.24.10 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pymupdf) (1.24.10)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (1.26.4)\n",
      "Requirement already satisfied: datasets in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (2.21.0)\n",
      "Requirement already satisfied: tiktoken in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (0.7.0)\n",
      "Requirement already satisfied: langchain in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (0.2.16)\n",
      "Requirement already satisfied: langchain-community in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (0.2.17)\n",
      "Requirement already satisfied: langchain-openai in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (0.1.25)\n",
      "Requirement already satisfied: openai>1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (1.44.1)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (0.3.4)\n",
      "Requirement already satisfied: nest-asyncio in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: Jinja2>3 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe<3.0,>=2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (2.1.5)\n",
      "Requirement already satisfied: PyYAML<7,>=5.4.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (6.0.1)\n",
      "Requirement already satisfied: Pygments<3.0.0,>=2.10.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (2.18.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.0.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (8.1.7)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.6 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (0.5.14)\n",
      "Requirement already satisfied: htmlmin<0.2.0,>=0.1.12 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (0.1.12)\n",
      "Requirement already satisfied: markdown2<3.0.0,>=2.4.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (2.5.0)\n",
      "Requirement already satisfied: pytz in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (2024.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (2.32.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.16.0->docling-core) (0.18.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain-text-splitters) (0.1.120)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain-text-splitters) (23.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain-text-splitters) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain-text-splitters) (4.11.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai>1->ragas) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai>1->ragas) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai>1->ragas) (0.5.0)\n",
      "Requirement already satisfied: sniffio in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from openai>1->ragas) (4.66.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.2->docling-core) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pandas<3.0.0,>=2.2.2->docling-core) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.0->docling-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.6.0->docling-core) (2.20.1)\n",
      "Requirement already satisfied: setuptools>=42 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pyproject-toml<0.0.11,>=0.0.10->docling-core) (69.5.1)\n",
      "Requirement already satisfied: wheel in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pyproject-toml<0.0.11,>=0.0.10->docling-core) (0.43.0)\n",
      "Requirement already satisfied: toml in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from pyproject-toml<0.0.11,>=0.0.10->docling-core) (0.10.2)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets->ragas) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from datasets->ragas) (0.23.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langchain->ragas) (2.0.32)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from tiktoken->ragas) (2024.5.15)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from aiohttp->datasets->ragas) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.7)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.6->json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from dataclasses-json<0.6.0,>=0.5.6->json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (0.9.0)\n",
      "Requirement already satisfied: certifi in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain-text-splitters) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.38->langchain-text-splitters) (3.10.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.2->docling-core) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from requests<3.0.0,>=2.31.0->json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (2.2.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/miniconda3/envs/llmops-course/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.6->json-schema-for-humans<2.0.0,>=1.0.0->docling-core) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install docling --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install docling-core python-dotenv langchain-text-splitters pymupdf4llm pymupdf ragas\n",
    "%pip install -qU langsmith langchain-core langchain-community langchain-openai langchain-qdrant langchain_experimental "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\", category=UserWarning, module=\"pydantic|torch\")\n",
    "warnings.filterwarnings(action=\"ignore\", category=FutureWarning, module=\"easyocr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"PDF Loader Comparison\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"https://arxiv.org/pdf/2206.01062\"\n",
    "# FILE_PATH=\"https://s2.q4cdn.com/470004039/files/doc_earnings/2024/q3/filing/_10-Q-Q3-2024-As-Filed.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.tools.arxiv.tool import ArxivQueryRun, arxiv\n",
    "# from langchain_community.tools.arxiv import arxiv\n",
    "# arxiv_query = ArxivQueryRun()\n",
    "# arxiv_paper = arxiv_query.invoke(FILE_PATH)\n",
    "# arxiv_paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from ragas.testset.docstore import Document, DocumentStore,InMemoryDocumentStore\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "llm_as_generator = \"gpt-4o\"\n",
    "generator_llm = ChatOpenAI(model=llm_as_generator)\n",
    "llm_as_critic = \"gpt-4o\"\n",
    "critic_llm = ChatOpenAI(model=llm_as_critic)\n",
    "embedding_model_for_generator = \"text-embedding-3-large\"\n",
    "embedding_model_dimensions = 3072\n",
    "embeddings = OpenAIEmbeddings(model=embedding_model_for_generator,dimensions=embedding_model_dimensions)\n",
    "\n",
    "# splitter = SemanticChunker(embeddings)\n",
    "# docstore = InMemoryDocumentStore(splitter,)\n",
    "\n",
    "generator = TestsetGenerator.from_langchain( # Default uses TokenTextSplitter\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")\n",
    "\n",
    "distributions = {\n",
    "    simple: 0.4,\n",
    "    multi_context: 0.4,\n",
    "    reasoning: 0.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic testset using raga\n",
    "We are going to use `PyMuPDFLoader` to read a pdf file for generating question. In the next iteration we will try and use advanced pdf loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "documents = PyMuPDFLoader(file_path=FILE_PATH).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://arxiv.org/pdf/2206.01062', 'file_path': 'https://arxiv.org/pdf/2206.01062', 'page': 0, 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis', 'author': 'Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar', 'subject': '-  Information systems  ->  Document structure; -  Applied computing  ->  Document analysis; -  Computing methodologies  ->  Machine learning; Computer vision; Object detection; ', 'keywords': 'PDF document conversion, layout segmentation, object-detection, data set, Machine Learning', 'creator': 'LaTeX with acmart 2018/04/14 v1.53 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220603003241Z', 'modDate': 'D:20220603003241Z', 'trapped': ''}, page_content=\"DocLayNet: A Large Human-Annotated Dataset for\\nDocument-Layout Analysis\\nBirgit Pfitzmann\\nIBM Research\\nRueschlikon, Switzerland\\nbpf@zurich.ibm.com\\nChristoph Auer\\nIBM Research\\nRueschlikon, Switzerland\\ncau@zurich.ibm.com\\nMichele Dolfi\\nIBM Research\\nRueschlikon, Switzerland\\ndol@zurich.ibm.com\\nAhmed S. Nassar\\nIBM Research\\nRueschlikon, Switzerland\\nahn@zurich.ibm.com\\nPeter Staar\\nIBM Research\\nRueschlikon, Switzerland\\ntaa@zurich.ibm.com\\nABSTRACT\\nAccurate document layout analysis is a key requirement for high-\\nquality PDF document conversion. With the recent availability of\\npublic, large ground-truth datasets such as PubLayNet and DocBank,\\ndeep-learning models have proven to be very effective at layout\\ndetection and segmentation. While these datasets are of adequate\\nsize to train such models, they severely lack in layout variability\\nsince they are sourced from scientific article repositories such as\\nPubMed and arXiv only. Consequently, the accuracy of the layout\\nsegmentation drops significantly when these models are applied\\non more challenging and diverse layouts. In this paper, we present\\nDocLayNet, a new, publicly available, document-layout annotation\\ndataset in COCO format. It contains 80863 manually annotated\\npages from diverse data sources to represent a wide variability in\\nlayouts. For each PDF page, the layout annotations provide labelled\\nbounding-boxes with a choice of 11 distinct classes. DocLayNet\\nalso provides a subset of double- and triple-annotated pages to\\ndetermine the inter-annotator agreement. In multiple experiments,\\nwe provide baseline accuracy scores (in mAP) for a set of popular\\nobject detection models. We also demonstrate that these models\\nfall approximately 10% behind the inter-annotator agreement. Fur-\\nthermore, we provide evidence that DocLayNet is of sufficient size.\\nLastly, we compare models trained on PubLayNet, DocBank and\\nDocLayNet, showing that layout predictions of the DocLayNet-\\ntrained models are more robust and thus the preferred choice for\\ngeneral-purpose document-layout analysis.\\nCCS CONCEPTS\\n• Information systems →Document structure; • Applied com-\\nputing →Document analysis; • Computing methodologies\\n→Machine learning; Computer vision; Object detection;\\nPermission to make digital or hard copies of part or all of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for third-party components of this work must be honored.\\nFor all other uses, contact the owner/author(s).\\nKDD ’22, August 14–18, 2022, Washington, DC, USA\\n© 2022 Copyright held by the owner/author(s).\\nACM ISBN 978-1-4503-9385-0/22/08.\\nhttps://doi.org/10.1145/3534678.3539043\\n13\\nUSING THE VERTICAL TUBE -\\nMODELS AY11230/11234\\n1. The vertical tube can be used for  \\n \\n \\n   instructional viewing or to photograph\\n \\n    the image with a digital camera or a\\n \\n \\n   micro TV unit\\n \\n2. Loosen the retention screw, then rotate \\n \\n    the adjustment ring to change the \\n \\n \\n \\n   length of the vertical tube.\\n3. Make sure that both the images in\\n \\nOPERATION (cont.)\\nSELECTING OBJECTIVE \\nMAGNIFICATION \\n \\n1. There are two objectives. The lower  \\n    magnification objective has a greater  \\n    depth of field and view.\\n2. In order to observe the specimen  \\n    easily use the lower magnification  \\n    objective first. Then, by rotating the  \\n    case, the magnification can be   \\n    changed.\\nCHANGING THE INTERPUPILLARY \\nDISTANCE\\n1. The distance between the observer's  \\n    pupils is the interpupillary distance.   \\n2. To adjust the interpupillary distance  \\n    rotate the prism caps until both eyes  \\n    coincide with the image in the   \\n    eyepiece. \\n \\nFOCUSING\\n1. Remove the lens protective cover.\\n2. Place the specimen on the working  \\n    stage.\\n3. Focus the specimen with the left eye  \\n    first while turning the focus knob until  \\n    the image appears clear and sharp.\\n4. Rotate the right eyepiece ring until the  \\n    images in each eyepiece coincide and  \\n    are sharp and clear.\\nCHANGING THE BULB\\n1. Disconnect the power cord.\\n2. When the bulb is cool, remove the  \\n    oblique illuminator cap and remove  \\n    the halogen bulb with cap.\\n3. Replace with a new halogen bulb.\\n4. Open the window in the base plate and  \\n    replace the halogen lamp or  \\n \\n    fluorescent lamp of transmitted   \\n    illuminator.\\nFOCUSING\\n1. Turn the focusing knob away or toward  \\n    you until a clear image is viewed.\\n2. If the image is unclear, adjust the  \\n    height of the elevator up or down,  \\n    then turn the focusing knob again.\\nZOOM MAGNIFICATION\\n1. Turn the zoom magnification knob to  \\n    the desired magnification and field of  \\n    view.\\n2. In most situations, it is recommended  \\n    that you focus at the lowest  \\n \\n    magnification, then move to a higher  \\n    magnification and re-focus as  \\n \\n    necessary.\\n3. If the image is not clear to both eyes  \\n    at the same time, the diopter ring may  \\n    need adjustment.\\nDIOPTER RING ADJUSTMENT\\n1. To adjust the eyepiece for viewing with  \\n    or without eyeglasses and for  \\n \\n    differences in acuity between the right  \\n    and left eyes, follow the following  \\n    steps:\\n    a. Observe an image through the left  \\n        eyepiece and bring a specific point  \\n        into focus using the focus knob.\\n    b. By turning the diopter ring  \\n \\n        adjustment for the left eyepiece,  \\n        bring the same point into sharp  \\n        focus.\\n     c.Then bring the same point into  \\n        focus through the right eyepiece   \\n        by turning the right diopter ring.\\n     d.With more than one viewer, each  \\n        viewer should note their own   \\n        diopter ring position for the left   \\n        and right eyepieces, then before  \\n        viewing set the diopter ring   \\n        adjustments to that setting.\\nCHANGING THE BULB\\n1. Disconnect the power cord from the  \\n    electrical outlet.\\n2. When the bulb is cool, remove the  \\n    oblique illuminator cap and remove  \\n    the halogen bulb with cap.\\n3. Replace with a new halogen bulb.\\n4. Open the window in the base plate   \\n    and replace the halogen lamp or  \\n    fluorescent lamp of transmitted   \\n    illuminator.\\n \\n \\n \\n \\n \\nModel AY11230\\nModel AY11234\\n14\\nObjectives\\nRevolving Turret\\nCoarse \\nAdjustment\\nKnob\\nMODEL AY11236\\nMICROSCOPE USAGE\\nBARSKA Model AY11236 is a powerful fixed power compound \\nmicroscope designed for biological studies such as specimen \\nexamination. It can also be used for examining bacteria and          \\nfor general clinical and medical studies and other scientific uses. \\nCONSTRUCTION\\nBARSKA Model AY11236 is a fixed power compound microscope.   \\nIt is constructed with two optical paths at the same angle. It is \\nequipped with transmitted illumination. By using this instrument, \\nthe user can observe specimens at magnification from 40x to \\n1000x by selecting the desired objective lens. Coarse and fine \\nfocus adjustments provide accuracy and image detail. The rotating \\nhead allows the user to position the eyepieces for maximum \\nviewing comfort and easy access to all adjustment knobs.  \\nModel AY11236\\nFine \\nAdjustment\\nKnob\\nStage\\nCondenser \\nFocusing\\nKnob\\nEyepiece\\nStand\\nLamp \\nOn/Off\\nSwitch\\nLamp \\nPower\\nCord\\nRotating Head\\nStage Clip\\nAdjustment\\nInterpupillary Slide Adjustment\\nCircling Minimums\\n7KHUH\\x03ZDV\\x03D\\x03FKDQJH\\x03WR\\x03WKH\\x037(536\\x03FULWHULD\\x03LQ\\x03\\x15\\x13\\x14\\x15\\x03WKDW\\x03DႇHFWV\\x03FLUFOLQJ\\x03DUHD\\x03GLPHQVLRQ\\x03E\\\\\\x03H[SDQGLQJ\\x03WKH\\x03DUHDV\\x03WR\\x03SURYLGH\\x03\\nimproved obstacle protection. To indicate that the new criteria had been applied to a given procedure, a \\n is placed on \\nthe circling line of minimums. The new circling tables and explanatory information is located in the Legend of the TPP.\\n7KH\\x03DSSURDFKHV\\x03XVLQJ\\x03VWDQGDUG\\x03FLUFOLQJ\\x03DSSURDFK\\x03DUHDV\\x03FDQ\\x03EH\\x03LGHQWL¿HG\\x03E\\\\\\x03WKH\\x03DEVHQFH\\x03RI\\x03WKH\\x03\\n on the circling line of \\nminima.\\n$SSO\\\\\\x036WDQGDUG\\x03&LUFOLQJ\\x03$SSURDFK\\x030DQHXYHULQJ\\x035DGLXV\\x037DEOH\\n$SSO\\\\\\x03([SDQGHG\\x03&LUFOLQJ\\x03$SSURDFK\\x030DQHXYHULQJ\\x03$LUVSDFH\\x035DGLXV\\x03\\nTable\\nAIRPORT SKETCH\\n \\n \\n                                                                                                                            \\nThe airport sketch is a depiction of the airport with emphasis on runway pattern and related \\ninformation, positioned in either the lower left or lower right corner of the chart to aid pi-\\nlot recognition of the airport from the air and to provide some information to aid on ground \\nnavigation of the airport. The runways are drawn to scale and oriented to true north. Runway \\ndimensions (length and width) are shown for all active runways.\\nRunway(s) are depicted based on what type and construction of the runway.\\nHard Surface\\nOther Than \\nHard Surface\\nMetal Surface\\nClosed Runway\\nUnder Construction\\nStopways, \\nTaxiways, Park-\\ning Areas\\nDisplaced \\nThreshold\\nClosed  \\nPavement\\nWater Runway\\nTaxiways and aprons are shaded grey. Other runway features that may be shown are runway numbers, runway dimen-\\nsions, runway slope, arresting gear, and displaced threshold.\\n2WKHU\\x03LQIRUPDWLRQ\\x03FRQFHUQLQJ\\x03OLJKWLQJ\\x0f\\x03¿QDO\\x03DSSURDFK\\x03EHDULQJV\\x0f\\x03DLUSRUW\\x03EHDFRQ\\x0f\\x03REVWDFOHV\\x0f\\x03FRQWURO\\x03WRZHU\\x0f\\x031$9$,'V\\x0f\\x03KHOL-\\npads may also be shown.\\n$LUSRUW\\x03(OHYDWLRQ\\x03DQG\\x037RXFKGRZQ\\x03=RQH\\x03(OHYDWLRQ\\nThe airport elevation is shown enclosed within a box in the upper left corner of the sketch box and the touchdown zone \\nelevation (TDZE) is shown in the upper right corner of the sketch box. The airport elevation is the highest point of an \\nDLUSRUW¶V\\x03XVDEOH\\x03UXQZD\\\\V\\x03PHDVXUHG\\x03LQ\\x03IHHW\\x03IURP\\x03PHDQ\\x03VHD\\x03OHYHO\\x11\\x037KH\\x037'=(\\x03LV\\x03WKH\\x03KLJKHVW\\x03HOHYDWLRQ\\x03LQ\\x03WKH\\x03¿UVW\\x03\\x16\\x0f\\x13\\x13\\x13\\x03IHHW\\x03RI\\x03\\nthe landing surface. Circling only approaches will not show a TDZE.\\n114\\nFAA Chart Users’ Guide - Terminal Procedures Publication (TPP) - Terms\\nAGL 2013 Financial Calendar\\n22 August 2012 \\n2012 full year result and ﬁnal dividend announced\\n30 August 2012 \\nEx-dividend trading commences\\n5 September 2012 \\nRecord date for 2012 ﬁnal dividend\\n27 September 2012 \\nFinal dividend payable\\n23 October 2012 \\nAnnual General Meeting\\n27 February 20131 \\n2013 interim result and interim dividend announced\\n28 August 20131 \\n2013 full year results and ﬁnal dividend announced \\n1 Indicative dates only, subject to change/Board conﬁrmation\\nAGL’s Annual General Meeting will be held at the City Recital Hall, Angel Place, Sydney \\ncommencing at 10.30am on Tuesday 23 October 2012.\\nYesterday\\nEstablished in Sydney in 1837, and then \\nknown as The Australian Gas Light Company, \\nthe AGL business has an established history \\nand reputation for serving the gas and \\nelectricity needs of Australian households. \\nIn 1841, when AGL supplied the gas to light \\nthe ﬁrst public street lamp, it was reported \\nin the Sydney Gazette as a “wonderful \\nachievement of scientiﬁc knowledge, assisted \\nby mechanical ingenuity.” Within two years, \\n165 gas lamps were lighting the City of Sydney.\\nLooking back on \\n175\\xa0years of \\nlooking\\xa0forward.\\nAGL Energy Limited ABN 74 115 061 375\\n29\\nsigns, signals and road markings\\n3\\nIn chapter 2, you and your vehicle, you learned about \\nsome of the controls in your vehicle. This chapter is a handy \\nreference section that gives examples of the most common \\nsigns, signals and road markings that keep trafﬁc organized \\nand ﬂowing smoothly. \\nSigns\\nThere are three ways to read signs: by their shape, colour and \\nthe messages printed on them. Understanding these three ways \\nof classifying signs will help you ﬁgure out the meaning of signs \\nthat are new to you. \\nStop\\nYield the right-of-way\\nShows driving  \\nregulations\\nExplains lane use\\nSchool zone signs \\nare ﬂuorescent \\nyellow-green\\nTells about motorist \\nservices\\nShows a permitted \\naction\\nShows an action that \\nis not permitted\\nWarns of hazards \\nahead\\nWarns of  \\nconstruction zones\\nRailway crossing\\nShows distance and \\ndirection\\n• Signs\\n – regulatory signs\\n – school, \\nplayground and \\ncrosswalk signs\\n – lane use signs\\n –  turn control signs\\n –  parking signs\\n –  reserved lane \\nsigns\\n –  warning signs\\n –  object markers\\n –  construction \\nsigns\\n – information and \\ndestination signs\\n –  railway signs\\n• Signals\\n – lane control \\nsignals\\n – trafﬁc lights\\n• Road markings\\n – yellow lines\\n – white lines\\n – reserved lane \\nmarkings\\n – other markings\\nin this chapter\\nFigure 1: Four examples of complex page layouts across dif-\\nferent document categories\\nKEYWORDS\\nPDF document conversion, layout segmentation, object-detection,\\ndata set, Machine Learning\\nACM Reference Format:\\nBirgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter\\nStaar. 2022. DocLayNet: A Large Human-Annotated Dataset for Document-\\nLayout Analysis. In Proceedings of the 28th ACM SIGKDD Conference on\\nKnowledge Discovery and Data Mining (KDD ’22), August 14–18, 2022, Wash-\\nington, DC, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/\\n3534678.3539043\\narXiv:2206.01062v1  [cs.CV]  2 Jun 2022\\n\"),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2206.01062', 'file_path': 'https://arxiv.org/pdf/2206.01062', 'page': 1, 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis', 'author': 'Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar', 'subject': '-  Information systems  ->  Document structure; -  Applied computing  ->  Document analysis; -  Computing methodologies  ->  Machine learning; Computer vision; Object detection; ', 'keywords': 'PDF document conversion, layout segmentation, object-detection, data set, Machine Learning', 'creator': 'LaTeX with acmart 2018/04/14 v1.53 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220603003241Z', 'modDate': 'D:20220603003241Z', 'trapped': ''}, page_content='KDD ’22, August 14–18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\n1\\nINTRODUCTION\\nDespite the substantial improvements achieved with machine-learning\\n(ML) approaches and deep neural networks in recent years, docu-\\nment conversion remains a challenging problem, as demonstrated\\nby the numerous public competitions held on this topic [1–4]. The\\nchallenge originates from the huge variability in PDF documents\\nregarding layout, language and formats (scanned, programmatic\\nor a combination of both). Engineering a single ML model that can\\nbe applied on all types of documents and provides high-quality\\nlayout segmentation remains to this day extremely challenging [5].\\nTo highlight the variability in document layouts, we show a few\\nexample documents from the DocLayNet dataset in Figure 1.\\nA key problem in the process of document conversion is to under-\\nstand the structure of a single document page, i.e. which segments\\nof text should be grouped together in a unit. To train models for this\\ntask, there are currently two large datasets available to the com-\\nmunity, PubLayNet [6] and DocBank [7]. They were introduced\\nin 2019 and 2020 respectively and significantly accelerated the im-\\nplementation of layout detection and segmentation models due to\\ntheir sizes of 300K and 500K ground-truth pages. These sizes were\\nachieved by leveraging an automation approach. The benefit of au-\\ntomated ground-truth generation is obvious: one can generate large\\nground-truth datasets at virtually no cost. However, the automation\\nintroduces a constraint on the variability in the dataset, because\\ncorresponding structured source data must be available. PubLayNet\\nand DocBank were both generated from scientific document repos-\\nitories (PubMed and arXiv), which provide XML or LATEX sources.\\nThose scientific documents present a limited variability in their\\nlayouts, because they are typeset in uniform templates provided by\\nthe publishers. Obviously, documents such as technical manuals,\\nannual company reports, legal text, government tenders, etc. have\\nvery different and partially unique layouts. As a consequence, the\\nlayout predictions obtained from models trained on PubLayNet or\\nDocBank is very reasonable when applied on scientific documents.\\nHowever, for more artistic or free-style layouts, we see sub-par\\nprediction quality from these models, which we demonstrate in\\nSection 5.\\nIn this paper, we present the DocLayNet dataset. It provides page-\\nby-page layout annotation ground-truth using bounding-boxes for\\n11 distinct class labels on 80863 unique document pages, of which\\na fraction carry double- or triple-annotations. DocLayNet is similar\\nin spirit to PubLayNet and DocBank and will likewise be made\\navailable to the public1 in order to stimulate the document-layout\\nanalysis community. It distinguishes itself in the following aspects:\\n(1) Human Annotation: In contrast to PubLayNet and DocBank,\\nwe relied on human annotation instead of automation ap-\\nproaches to generate the data set.\\n(2) Large Layout Variability: We include diverse and complex\\nlayouts from a large variety of public sources.\\n(3) Detailed Label Set: We define 11 class labels to distinguish\\nlayout features in high detail. PubLayNet provides 5 labels;\\nDocBank provides 13, although not a superset of ours.\\n(4) Redundant Annotations: A fraction of the pages in the Do-\\ncLayNet data set carry more than one human annotation.\\n1https://developer.ibm.com/exchanges/data/all/doclaynet\\nThis enables experimentation with annotation uncertainty\\nand quality control analysis.\\n(5) Pre-defined Train-, Test- & Validation-set: Like DocBank, we\\nprovide fixed train-, test- & validation-sets to ensure propor-\\ntional representation of the class-labels. Further, we prevent\\nleakage of unique layouts across sets, which has a large effect\\non model accuracy scores.\\nAll aspects outlined above are detailed in Section 3. In Section 4,\\nwe will elaborate on how we designed and executed this large-scale\\nhuman annotation campaign. We will also share key insights and\\nlessons learned that might prove helpful for other parties planning\\nto set up annotation campaigns.\\nIn Section 5, we will present baseline accuracy numbers for a\\nvariety of object detection methods (Faster R-CNN, Mask R-CNN\\nand YOLOv5) trained on DocLayNet. We further show how the\\nmodel performance is impacted by varying the DocLayNet dataset\\nsize, reducing the label set and modifying the train/test-split. Last\\nbut not least, we compare the performance of models trained on\\nPubLayNet, DocBank and DocLayNet and demonstrate that a model\\ntrained on DocLayNet provides overall more robust layout recovery.\\n2\\nRELATED WORK\\nWhile early approaches in document-layout analysis used rule-\\nbased algorithms and heuristics [8], the problem is lately addressed\\nwith deep learning methods. The most common approach is to lever-\\nage object detection models [9–15]. In the last decade, the accuracy\\nand speed of these models has increased dramatically. Furthermore,\\nmost state-of-the-art object detection methods can be trained and\\napplied with very little work, thanks to a standardisation effort\\nof the ground-truth data format [16] and common deep-learning\\nframeworks [17]. Reference data sets such as PubLayNet [6] and\\nDocBank provide their data in the commonly accepted COCO for-\\nmat [16].\\nLately, new types of ML models for document-layout analysis\\nhave emerged in the community [18–21]. These models do not\\napproach the problem of layout analysis purely based on an image\\nrepresentation of the page, as computer vision methods do. Instead,\\nthey combine the text tokens and image representation of a page\\nin order to obtain a segmentation. While the reported accuracies\\nappear to be promising, a broadly accepted data format which links\\ngeometric and textual features has yet to establish.\\n3\\nTHE DOCLAYNET DATASET\\nDocLayNet contains 80863 PDF pages. Among these, 7059 carry two\\ninstances of human annotations, and 1591 carry three. This amounts\\nto 91104 total annotation instances. The annotations provide lay-\\nout information in the shape of labeled, rectangular bounding-\\nboxes. We define 11 distinct labels for layout features, namely Cap-\\ntion, Footnote, Formula, List-item, Page-footer, Page-header, Picture,\\nSection-header, Table, Text, and Title. Our reasoning for picking this\\nparticular label set is detailed in Section 4.\\nIn addition to open intellectual property constraints for the\\nsource documents, we required that the documents in DocLayNet\\nadhere to a few conditions. Firstly, we kept scanned documents\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2206.01062', 'file_path': 'https://arxiv.org/pdf/2206.01062', 'page': 2, 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis', 'author': 'Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar', 'subject': '-  Information systems  ->  Document structure; -  Applied computing  ->  Document analysis; -  Computing methodologies  ->  Machine learning; Computer vision; Object detection; ', 'keywords': 'PDF document conversion, layout segmentation, object-detection, data set, Machine Learning', 'creator': 'LaTeX with acmart 2018/04/14 v1.53 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220603003241Z', 'modDate': 'D:20220603003241Z', 'trapped': ''}, page_content='DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\nKDD ’22, August 14–18, 2022, Washington, DC, USA\\nPatents\\n8%\\nScientific\\n17%\\nFinancial\\n32%\\nTenders\\n6%\\nLaws\\n16%\\nManuals\\n21%\\nFigure 2: Distribution of DocLayNet pages across document\\ncategories.\\nto a minimum, since they introduce difficulties in annotation (see\\nSection 4). As a second condition, we focussed on medium to large\\ndocuments (> 10 pages) with technical content, dense in complex\\ntables, figures, plots and captions. Such documents carry a lot of\\ninformation value, but are often hard to analyse with high accuracy\\ndue to their challenging layouts. Counterexamples of documents\\nnot included in the dataset are receipts, invoices, hand-written\\ndocuments or photographs showing “text in the wild\".\\nThe pages in DocLayNet can be grouped into six distinct cate-\\ngories, namely Financial Reports, Manuals, Scientific Articles, Laws &\\nRegulations, Patents and Government Tenders. Each document cate-\\ngory was sourced from various repositories. For example, Financial\\nReports contain both free-style format annual reports2 which ex-\\npose company-specific, artistic layouts as well as the more formal\\nSEC filings. The two largest categories (Financial Reports and Man-\\nuals) contain a large amount of free-style layouts in order to obtain\\nmaximum variability. In the other four categories, we boosted the\\nvariability by mixing documents from independent providers, such\\nas different government websites or publishers. In Figure 2, we\\nshow the document categories contained in DocLayNet with their\\nrespective sizes.\\nWe did not control the document selection with regard to lan-\\nguage. The vast majority of documents contained in DocLayNet\\n(close to 95%) are published in English language. However, Do-\\ncLayNet also contains a number of documents in other languages\\nsuch as German (2.5%), French (1.0%) and Japanese (1.0%). While\\nthe document language has negligible impact on the performance\\nof computer vision methods such as object detection and segmenta-\\ntion models, it might prove challenging for layout analysis methods\\nwhich exploit textual features.\\nTo ensure that future benchmarks in the document-layout analy-\\nsis community can be easily compared, we have split up DocLayNet\\ninto pre-defined train-, test- and validation-sets. In this way, we can\\navoid spurious variations in the evaluation scores due to random\\nsplitting in train-, test- and validation-sets. We also ensured that\\nless frequent labels are represented in train and test sets in equal\\nproportions.\\n2e.g. AAPL from https://www.annualreports.com/\\nTable 1 shows the overall frequency and distribution of the labels\\namong the different sets. Importantly, we ensure that subsets are\\nonly split on full-document boundaries. This avoids that pages of\\nthe same document are spread over train, test and validation set,\\nwhich can give an undesired evaluation advantage to models and\\nlead to overestimation of their prediction accuracy. We will show\\nthe impact of this decision in Section 5.\\nIn order to accommodate the different types of models currently\\nin use by the community, we provide DocLayNet in an augmented\\nCOCO format [16]. This entails the standard COCO ground-truth\\nfile (in JSON format) with the associated page images (in PNG\\nformat, 1025×1025 pixels). Furthermore, custom fields have been\\nadded to each COCO record to specify document category, original\\ndocument filename and page number. In addition, we also provide\\nthe original PDF pages, as well as sidecar files containing parsed\\nPDF text and text-cell coordinates (in JSON). All additional files are\\nlinked to the primary page images by their matching filenames.\\nDespite being cost-intense and far less scalable than automation,\\nhuman annotation has several benefits over automated ground-\\ntruth generation. The first and most obvious reason to leverage\\nhuman annotations is the freedom to annotate any type of doc-\\nument without requiring a programmatic source. For most PDF\\ndocuments, the original source document is not available. The lat-\\nter is not a hard constraint with human annotation, but it is for\\nautomated methods. A second reason to use human annotations is\\nthat the latter usually provide a more natural interpretation of the\\npage layout. The human-interpreted layout can significantly devi-\\nate from the programmatic layout used in typesetting. For example,\\n“invisible” tables might be used solely for aligning text paragraphs\\non columns. Such typesetting tricks might be interpreted by au-\\ntomated methods incorrectly as an actual table, while the human\\nannotation will interpret it correctly as Text or other styles. The\\nsame applies to multi-line text elements, when authors decided to\\nspace them as “invisible” list elements without bullet symbols. A\\nthird reason to gather ground-truth through human annotation is\\nto estimate a “natural” upper bound on the segmentation accuracy.\\nAs we will show in Section 4, certain documents featuring complex\\nlayouts can have different but equally acceptable layout interpre-\\ntations. This natural upper bound for segmentation accuracy can\\nbe found by annotating the same pages multiple times by different\\npeople and evaluating the inter-annotator agreement. Such a base-\\nline consistency evaluation is very useful to define expectations\\nfor a good target accuracy in trained deep neural network models\\nand avoid overfitting (see Table 1). On the flip side, achieving high\\nannotation consistency proved to be a key challenge in human\\nannotation, as we outline in Section 4.\\n4\\nANNOTATION CAMPAIGN\\nThe annotation campaign was carried out in four phases. In phase\\none, we identified and prepared the data sources for annotation.\\nIn phase two, we determined the class labels and how annotations\\nshould be done on the documents in order to obtain maximum con-\\nsistency. The latter was guided by a detailed requirement analysis\\nand exhaustive experiments. In phase three, we trained the annota-\\ntion staff and performed exams for quality assurance. In phase four,\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2206.01062', 'file_path': 'https://arxiv.org/pdf/2206.01062', 'page': 3, 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis', 'author': 'Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar', 'subject': '-  Information systems  ->  Document structure; -  Applied computing  ->  Document analysis; -  Computing methodologies  ->  Machine learning; Computer vision; Object detection; ', 'keywords': 'PDF document conversion, layout segmentation, object-detection, data set, Machine Learning', 'creator': 'LaTeX with acmart 2018/04/14 v1.53 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220603003241Z', 'modDate': 'D:20220603003241Z', 'trapped': ''}, page_content='KDD ’22, August 14–18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\nTable 1: DocLayNet dataset overview. Along with the frequency of each class label, we present the relative occurrence (as %\\nof row “Total”) in the train, test and validation sets. The inter-annotator agreement is computed as the mAP@0.5-0.95 metric\\nbetween pairwise annotations from the triple-annotated pages, from which we obtain accuracy ranges.\\n% of Total\\ntriple inter-annotator mAP @ 0.5-0.95 (%)\\nclass label\\nCount\\nTrain\\nTest\\nVal\\nAll\\nFin\\nMan\\nSci\\nLaw\\nPat\\nTen\\nCaption\\n22524\\n2.04\\n1.77\\n2.32\\n84-89\\n40-61\\n86-92\\n94-99\\n95-99\\n69-78\\nn/a\\nFootnote\\n6318\\n0.60\\n0.31\\n0.58\\n83-91\\nn/a\\n100\\n62-88\\n85-94\\nn/a\\n82-97\\nFormula\\n25027\\n2.25\\n1.90\\n2.96\\n83-85\\nn/a\\nn/a\\n84-87\\n86-96\\nn/a\\nn/a\\nList-item\\n185660\\n17.19\\n13.34\\n15.82\\n87-88\\n74-83\\n90-92\\n97-97\\n81-85\\n75-88\\n93-95\\nPage-footer\\n70878\\n6.51\\n5.58\\n6.00\\n93-94\\n88-90\\n95-96\\n100\\n92-97\\n100\\n96-98\\nPage-header\\n58022\\n5.10\\n6.70\\n5.06\\n85-89\\n66-76\\n90-94\\n98-100\\n91-92\\n97-99\\n81-86\\nPicture\\n45976\\n4.21\\n2.78\\n5.31\\n69-71\\n56-59\\n82-86\\n69-82\\n80-95\\n66-71\\n59-76\\nSection-header\\n142884\\n12.60\\n15.77\\n12.85\\n83-84\\n76-81\\n90-92\\n94-95\\n87-94\\n69-73\\n78-86\\nTable\\n34733\\n3.20\\n2.27\\n3.60\\n77-81\\n75-80\\n83-86\\n98-99\\n58-80\\n79-84\\n70-85\\nText\\n510377\\n45.82\\n49.28\\n45.00\\n84-86\\n81-86\\n88-93\\n89-93\\n87-92\\n71-79\\n87-95\\nTitle\\n5071\\n0.47\\n0.30\\n0.50\\n60-72\\n24-63\\n50-63\\n94-100\\n82-96\\n68-79\\n24-56\\nTotal\\n1107470\\n941123\\n99816\\n66531\\n82-83\\n71-74\\n79-81\\n89-94\\n86-91\\n71-76\\n68-85\\nFigure 3: Corpus Conversion Service annotation user inter-\\nface. The PDF page is shown in the background, with over-\\nlaid text-cells (in darker shades). The annotation boxes can\\nbe drawn by dragging a rectangle over each segment with\\nthe respective label from the palette on the right.\\nwe distributed the annotation workload and performed continuous\\nquality controls. Phase one and two required a small team of experts\\nonly. For phases three and four, a group of 40 dedicated annotators\\nwere assembled and supervised.\\nPhase 1: Data selection and preparation. Our inclusion cri-\\nteria for documents were described in Section 3. A large effort went\\ninto ensuring that all documents are free to use. The data sources\\ninclude publication repositories such as arXiv3, government offices,\\ncompany websites as well as data directory services for financial\\nreports and patents. Scanned documents were excluded wherever\\npossible because they can be rotated or skewed. This would not\\nallow us to perform annotation with rectangular bounding-boxes\\nand therefore complicate the annotation process.\\nPreparation work included uploading and parsing the sourced\\nPDF documents in the Corpus Conversion Service (CCS) [22], a\\ncloud-native platform which provides a visual annotation interface\\nand allows for dataset inspection and analysis. The annotation in-\\nterface of CCS is shown in Figure 3. The desired balance of pages\\nbetween the different document categories was achieved by se-\\nlective subsampling of pages with certain desired properties. For\\nexample, we made sure to include the title page of each document\\nand bias the remaining page selection to those with figures or\\ntables. The latter was achieved by leveraging pre-trained object\\ndetection models from PubLayNet, which helped us estimate how\\nmany figures and tables a given page contains.\\nPhase 2: Label selection and guideline. We reviewed the col-\\nlected documents and identified the most common structural fea-\\ntures they exhibit. This was achieved by identifying recurrent layout\\nelements and lead us to the definition of 11 distinct class labels.\\nThese 11 class labels are Caption, Footnote, Formula, List-item, Page-\\nfooter, Page-header, Picture, Section-header, Table, Text, and Title.\\nCritical factors that were considered for the choice of these class\\nlabels were (1) the overall occurrence of the label, (2) the specificity\\nof the label, (3) recognisability on a single page (i.e. no need for\\ncontext from previous or next page) and (4) overall coverage of the\\npage. Specificity ensures that the choice of label is not ambiguous,\\nwhile coverage ensures that all meaningful items on a page can\\nbe annotated. We refrained from class labels that are very specific\\nto a document category, such as Abstract in the Scientific Articles\\ncategory. We also avoided class labels that are tightly linked to the\\nsemantics of the text. Labels such as Author and Affiliation, as seen\\nin DocBank, are often only distinguishable by discriminating on\\n3https://arxiv.org/\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2206.01062', 'file_path': 'https://arxiv.org/pdf/2206.01062', 'page': 4, 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis', 'author': 'Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar', 'subject': '-  Information systems  ->  Document structure; -  Applied computing  ->  Document analysis; -  Computing methodologies  ->  Machine learning; Computer vision; Object detection; ', 'keywords': 'PDF document conversion, layout segmentation, object-detection, data set, Machine Learning', 'creator': 'LaTeX with acmart 2018/04/14 v1.53 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220603003241Z', 'modDate': 'D:20220603003241Z', 'trapped': ''}, page_content='DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\nKDD ’22, August 14–18, 2022, Washington, DC, USA\\nthe textual content of an element, which goes beyond visual layout\\nrecognition, in particular outside the Scientific Articles category.\\nAt first sight, the task of visual document-layout interpretation\\nappears intuitive enough to obtain plausible annotations in most\\ncases. However, during early trial-runs in the core team, we ob-\\nserved many cases in which annotators use different annotation\\nstyles, especially for documents with challenging layouts. For ex-\\nample, if a figure is presented with subfigures, one annotator might\\ndraw a single figure bounding-box, while another might annotate\\neach subfigure separately. The same applies for lists, where one\\nmight annotate all list items in one block or each list item sep-\\narately. In essence, we observed that challenging layouts would\\nbe annotated in different but plausible ways. To illustrate this, we\\nshow in Figure 4 multiple examples of plausible but inconsistent\\nannotations on the same pages.\\nObviously, this inconsistency in annotations is not desirable for\\ndatasets which are intended to be used for model training. To min-\\nimise these inconsistencies, we created a detailed annotation guide-\\nline. While perfect consistency across 40 annotation staff members\\nis clearly not possible to achieve, we saw a huge improvement in\\nannotation consistency after the introduction of our annotation\\nguideline. A few selected, non-trivial highlights of the guideline\\nare:\\n(1) Every list-item is an individual object instance with class\\nlabel List-item. This definition is different from PubLayNet\\nand DocBank, where all list-items are grouped together into\\none List object.\\n(2) A List-item is a paragraph with hanging indentation. Single-\\nline elements can qualify as List-item if the neighbour ele-\\nments expose hanging indentation. Bullet or enumeration\\nsymbols are not a requirement.\\n(3) For every Caption, there must be exactly one corresponding\\nPicture or Table.\\n(4) Connected sub-pictures are grouped together in one Picture\\nobject.\\n(5) Formula numbers are included in a Formula object.\\n(6) Emphasised text (e.g. in italic or bold) at the beginning of\\na paragraph is not considered a Section-header, unless it\\nappears exclusively on its own line.\\nThe complete annotation guideline is over 100 pages long and a\\ndetailed description is obviously out of scope for this paper. Never-\\ntheless, it will be made publicly available alongside with DocLayNet\\nfor future reference.\\nPhase 3: Training. After a first trial with a small group of peo-\\nple, we realised that providing the annotation guideline and a set of\\nrandom practice pages did not yield the desired quality level for lay-\\nout annotation. Therefore we prepared a subset of pages with two\\ndifferent complexity levels, each with a practice and an exam part.\\n974 pages were reference-annotated by one proficient core team\\nmember. Annotation staff were then given the task to annotate the\\nsame subsets (blinded from the reference). By comparing the an-\\nnotations of each staff member with the reference annotations, we\\ncould quantify how closely their annotations matched the reference.\\nOnly after passing two exam levels with high annotation quality,\\nstaff were admitted into the production phase. Practice iterations\\n1ef23f5e6d7f10d393f9947e8208285dce9ae87250ac483ac4b4a59d51b4e037\\nCompliant with guidelines\\nPlausible but invalid alternative\\nBorderline case: Two guideline-compliant alternatives\\n03c31a2ee1ed1b583c28957f475ee545d144e1b5a264dc4dd068c8d2f6a64860\\n1a5cd524f1844c1260c8e8c073e1f442423c264583212b0d0b6626fc780e6ed4\\n05237a14f2524e3f53c8454b074409d05078038a6a36b770fcc8ec7e540deae0\\nA\\nB\\nC\\nD\\nFigure 4: Examples of plausible annotation alternatives for\\nthe same page. Criteria in our annotation guideline can re-\\nsolve cases A to C, while the case D remains ambiguous.\\nwere carried out over a timeframe of 12 weeks, after which 8 of the\\n40 initially allocated annotators did not pass the bar.\\nPhase 4: Production annotation. The previously selected 80K\\npages were annotated with the defined 11 class labels by 32 annota-\\ntors. This production phase took around three months to complete.\\nAll annotations were created online through CCS, which visualises\\nthe programmatic PDF text-cells as an overlay on the page. The page\\nannotation are obtained by drawing rectangular bounding-boxes,\\nas shown in Figure 3. With regard to the annotation practices, we\\nimplemented a few constraints and capabilities on the tooling level.\\nFirst, we only allow non-overlapping, vertically oriented, rectangu-\\nlar boxes. For the large majority of documents, this constraint was\\nsufficient and it speeds up the annotation considerably in compar-\\nison with arbitrary segmentation shapes. Second, annotator staff\\nwere not able to see each other’s annotations. This was enforced by\\ndesign to avoid any bias in the annotation, which could skew the\\nnumbers of the inter-annotator agreement (see Table 1). We wanted\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2206.01062', 'file_path': 'https://arxiv.org/pdf/2206.01062', 'page': 5, 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis', 'author': 'Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar', 'subject': '-  Information systems  ->  Document structure; -  Applied computing  ->  Document analysis; -  Computing methodologies  ->  Machine learning; Computer vision; Object detection; ', 'keywords': 'PDF document conversion, layout segmentation, object-detection, data set, Machine Learning', 'creator': 'LaTeX with acmart 2018/04/14 v1.53 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220603003241Z', 'modDate': 'D:20220603003241Z', 'trapped': ''}, page_content='KDD ’22, August 14–18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\nTable 2: Prediction performance (mAP@0.5-0.95) of object\\ndetection networks on DocLayNet test set. The MRCNN\\n(Mask R-CNN) and FRCNN (Faster R-CNN) models with\\nResNet-50 or ResNet-101 backbone were trained based on\\nthe network architectures from the detectron2 model zoo\\n(Mask R-CNN R50, R101-FPN 3x, Faster R-CNN R101-FPN\\n3x), with default configurations. The YOLO implementation\\nutilized was YOLOv5x6 [13]. All models were initialised us-\\ning pre-trained weights from the COCO 2017 dataset.\\nhuman\\nMRCNN\\nFRCNN\\nYOLO\\nR50\\nR101\\nR101\\nv5x6\\nCaption\\n84-89\\n68.4\\n71.5\\n70.1\\n77.7\\nFootnote\\n83-91\\n70.9\\n71.8\\n73.7\\n77.2\\nFormula\\n83-85\\n60.1\\n63.4\\n63.5\\n66.2\\nList-item\\n87-88\\n81.2\\n80.8\\n81.0\\n86.2\\nPage-footer\\n93-94\\n61.6\\n59.3\\n58.9\\n61.1\\nPage-header\\n85-89\\n71.9\\n70.0\\n72.0\\n67.9\\nPicture\\n69-71\\n71.7\\n72.7\\n72.0\\n77.1\\nSection-header\\n83-84\\n67.6\\n69.3\\n68.4\\n74.6\\nTable\\n77-81\\n82.2\\n82.9\\n82.2\\n86.3\\nText\\n84-86\\n84.6\\n85.8\\n85.4\\n88.1\\nTitle\\n60-72\\n76.7\\n80.4\\n79.9\\n82.7\\nAll\\n82-83\\n72.4\\n73.5\\n73.4\\n76.8\\nto avoid this at any cost in order to have clear, unbiased baseline\\nnumbers for human document-layout annotation. Third, we in-\\ntroduced the feature of snapping boxes around text segments to\\nobtain a pixel-accurate annotation and again reduce time and effort.\\nThe CCS annotation tool automatically shrinks every user-drawn\\nbox to the minimum bounding-box around the enclosed text-cells\\nfor all purely text-based segments, which excludes only Table and\\nPicture. For the latter, we instructed annotation staff to minimise\\ninclusion of surrounding whitespace while including all graphical\\nlines. A downside of snapping boxes to enclosed text cells is that\\nsome wrongly parsed PDF pages cannot be annotated correctly and\\nneed to be skipped. Fourth, we established a way to flag pages as\\nrejected for cases where no valid annotation according to the label\\nguidelines could be achieved. Example cases for this would be PDF\\npages that render incorrectly or contain layouts that are impossible\\nto capture with non-overlapping rectangles. Such rejected pages are\\nnot contained in the final dataset. With all these measures in place,\\nexperienced annotation staff managed to annotate a single page in\\na typical timeframe of 20s to 60s, depending on its complexity.\\n5\\nEXPERIMENTS\\nThe primary goal of DocLayNet is to obtain high-quality ML models\\ncapable of accurate document-layout analysis on a wide variety\\nof challenging layouts. As discussed in Section 2, object detection\\nmodels are currently the easiest to use, due to the standardisation\\nof ground-truth data in COCO format [16] and the availability of\\ngeneral frameworks such as detectron2 [17]. Furthermore, baseline\\nnumbers in PubLayNet and DocBank were obtained using standard\\nobject detection models such as Mask R-CNN and Faster R-CNN.\\nAs such, we will relate to these object detection methods in this\\n0\\n20\\n40\\n60\\n80\\n100\\n% of DocLayNet training set\\n50\\n55\\n60\\n65\\n70\\nmAP 0.50:0.95\\n101\\n102\\n50\\n55\\n60\\n65\\n70\\nFigure 5: Prediction performance (mAP@0.5-0.95) of a Mask\\nR-CNN network with ResNet50 backbone trained on increas-\\ning fractions of the DocLayNet dataset. The learning curve\\nflattens around the 80% mark, indicating that increasing the\\nsize of the DocLayNet dataset with similar data will not yield\\nsignificantly better predictions.\\npaper and leave the detailed evaluation of more recent methods\\nmentioned in Section 2 for future work.\\nIn this section, we will present several aspects related to the\\nperformance of object detection models on DocLayNet. Similarly\\nas in PubLayNet, we will evaluate the quality of their predictions\\nusing mean average precision (mAP) with 10 overlaps that range\\nfrom 0.5 to 0.95 in steps of 0.05 (mAP@0.5-0.95). These scores are\\ncomputed by leveraging the evaluation code provided by the COCO\\nAPI [16].\\nBaselines for Object Detection\\nIn Table 2, we present baseline experiments (given in mAP) on Mask\\nR-CNN [12], Faster R-CNN [11], and YOLOv5 [13]. Both training\\nand evaluation were performed on RGB images with dimensions of\\n1025×1025 pixels. For training, we only used one annotation in case\\nof redundantly annotated pages. As one can observe, the variation\\nin mAP between the models is rather low, but overall between 6\\nand 10% lower than the mAP computed from the pairwise human\\nannotations on triple-annotated pages. This gives a good indication\\nthat the DocLayNet dataset poses a worthwhile challenge for the\\nresearch community to close the gap between human recognition\\nand ML approaches. It is interesting to see that Mask R-CNN and\\nFaster R-CNN produce very comparable mAP scores, indicating\\nthat pixel-based image segmentation derived from bounding-boxes\\ndoes not help to obtain better predictions. On the other hand, the\\nmore recent Yolov5x model does very well and even out-performs\\nhumans on selected labels such as Text, Table and Picture. This is\\nnot entirely surprising, as Text, Table and Picture are abundant and\\nthe most visually distinctive in a document.\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2206.01062', 'file_path': 'https://arxiv.org/pdf/2206.01062', 'page': 6, 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis', 'author': 'Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar', 'subject': '-  Information systems  ->  Document structure; -  Applied computing  ->  Document analysis; -  Computing methodologies  ->  Machine learning; Computer vision; Object detection; ', 'keywords': 'PDF document conversion, layout segmentation, object-detection, data set, Machine Learning', 'creator': 'LaTeX with acmart 2018/04/14 v1.53 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220603003241Z', 'modDate': 'D:20220603003241Z', 'trapped': ''}, page_content='DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\nKDD ’22, August 14–18, 2022, Washington, DC, USA\\nTable 3: Performance of a Mask R-CNN R50 network in\\nmAP@0.5-0.95 scores trained on DocLayNet with different\\nclass label sets. The reduced label sets were obtained by ei-\\nther down-mapping or dropping labels.\\nClass-count\\n11\\n6\\n5\\n4\\nCaption\\n68\\nText\\nText\\nText\\nFootnote\\n71\\nText\\nText\\nText\\nFormula\\n60\\nText\\nText\\nText\\nList-item\\n81\\nText\\n82\\nText\\nPage-footer\\n62\\n62\\n-\\n-\\nPage-header\\n72\\n68\\n-\\n-\\nPicture\\n72\\n72\\n72\\n72\\nSection-header\\n68\\n67\\n69\\n68\\nTable\\n82\\n83\\n82\\n82\\nText\\n85\\n84\\n84\\n84\\nTitle\\n77\\nSec.-h.\\nSec.-h.\\nSec.-h.\\nOverall\\n72\\n73\\n78\\n77\\nLearning Curve\\nOne of the fundamental questions related to any dataset is if it is\\n“large enough”. To answer this question for DocLayNet, we per-\\nformed a data ablation study in which we evaluated a Mask R-CNN\\nmodel trained on increasing fractions of the DocLayNet dataset.\\nAs can be seen in Figure 5, the mAP score rises sharply in the be-\\nginning and eventually levels out. To estimate the error-bar on the\\nmetrics, we ran the training five times on the entire data-set. This\\nresulted in a 1% error-bar, depicted by the shaded area in Figure 5.\\nIn the inset of Figure 5, we show the exact same data-points, but\\nwith a logarithmic scale on the x-axis. As is expected, the mAP\\nscore increases linearly as a function of the data-size in the inset.\\nThe curve ultimately flattens out between the 80% and 100% mark,\\nwith the 80% mark falling within the error-bars of the 100% mark.\\nThis provides a good indication that the model would not improve\\nsignificantly by yet increasing the data size. Rather, it would prob-\\nably benefit more from improved data consistency (as discussed\\nin Section 3), data augmentation methods [23], or the addition of\\nmore document categories and styles.\\nImpact of Class Labels\\nThe choice and number of labels can have a significant effect on\\nthe overall model performance. Since PubLayNet, DocBank and\\nDocLayNet all have different label sets, it is of particular interest to\\nunderstand and quantify this influence of the label set on the model\\nperformance. We investigate this by either down-mapping labels\\ninto more common ones (e.g. Caption→Text) or excluding them\\nfrom the annotations entirely. Furthermore, it must be stressed\\nthat all mappings and exclusions were performed on the data be-\\nfore model training. In Table 3, we present the mAP scores for a\\nMask R-CNN R50 network on different label sets. Where a label\\nis down-mapped, we show its corresponding label, otherwise it\\nwas excluded. We present three different label sets, with 6, 5 and 4\\ndifferent labels respectively. The set of 5 labels contains the same\\nlabels as PubLayNet. However, due to the different definition of\\nTable 4: Performance of a Mask R-CNN R50 network with\\ndocument-wise and page-wise split for different label sets.\\nNaive page-wise split will result in ~10% point improve-\\nment.\\nClass-count\\n11\\n5\\nSplit\\nDoc\\nPage\\nDoc\\nPage\\nCaption\\n68\\n83\\nFootnote\\n71\\n84\\nFormula\\n60\\n66\\nList-item\\n81\\n88\\n82\\n88\\nPage-footer\\n62\\n89\\nPage-header\\n72\\n90\\nPicture\\n72\\n82\\n72\\n82\\nSection-header\\n68\\n83\\n69\\n83\\nTable\\n82\\n89\\n82\\n90\\nText\\n85\\n91\\n84\\n90\\nTitle\\n77\\n81\\nAll\\n72\\n84\\n78\\n87\\nlists in PubLayNet (grouped list-items) versus DocLayNet (separate\\nlist-items), the label set of size 4 is the closest to PubLayNet, in the\\nassumption that the List is down-mapped to Text in PubLayNet.\\nThe results in Table 3 show that the prediction accuracy on the\\nremaining class labels does not change significantly when other\\nclasses are merged into them. The overall macro-average improves\\nby around 5%, in particular when Page-footer and Page-header are\\nexcluded.\\nImpact of Document Split in Train and Test Set\\nMany documents in DocLayNet have a unique styling. In order\\nto avoid overfitting on a particular style, we have split the train-,\\ntest- and validation-sets of DocLayNet on document boundaries, i.e.\\nevery document contributes pages to only one set. To the best of\\nour knowledge, this was not considered in PubLayNet or DocBank.\\nTo quantify how this affects model performance, we trained and\\nevaluated a Mask R-CNN R50 model on a modified dataset version.\\nHere, the train-, test- and validation-sets were obtained by a ran-\\ndomised draw over the individual pages. As can be seen in Table 4,\\nthe difference in model performance is surprisingly large: page-\\nwise splitting gains ˜10% in mAP over the document-wise splitting.\\nThus, random page-wise splitting of DocLayNet can easily lead\\nto accidental overestimation of model performance and should be\\navoided.\\nDataset Comparison\\nThroughout this paper, we claim that DocLayNet’s wider variety of\\ndocument layouts leads to more robust layout detection models. In\\nTable 5, we provide evidence for that. We trained models on each\\nof the available datasets (PubLayNet, DocBank and DocLayNet)\\nand evaluated them on the test sets of the other datasets. Due to\\nthe different label sets and annotation styles, a direct comparison\\nis not possible. Hence, we focussed on the common labels among\\nthe datasets. Between PubLayNet and DocLayNet, these are Picture,\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2206.01062', 'file_path': 'https://arxiv.org/pdf/2206.01062', 'page': 7, 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis', 'author': 'Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar', 'subject': '-  Information systems  ->  Document structure; -  Applied computing  ->  Document analysis; -  Computing methodologies  ->  Machine learning; Computer vision; Object detection; ', 'keywords': 'PDF document conversion, layout segmentation, object-detection, data set, Machine Learning', 'creator': 'LaTeX with acmart 2018/04/14 v1.53 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220603003241Z', 'modDate': 'D:20220603003241Z', 'trapped': ''}, page_content='KDD ’22, August 14–18, 2022, Washington, DC, USA Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar\\nTable 5: Prediction Performance (mAP@0.5-0.95) of a Mask\\nR-CNN R50 network across the PubLayNet, DocBank & Do-\\ncLayNet data-sets. By evaluating on common label classes of\\neach dataset, we observe that the DocLayNet-trained model\\nhas much less pronounced variations in performance across\\nall datasets.\\nTesting on\\nTraining on\\nlabels\\nPLN\\nDB\\nDLN\\nPubLayNet (PLN)\\nFigure\\n96\\n43\\n23\\nSec-header\\n87\\n-\\n32\\nTable\\n95\\n24\\n49\\nText\\n96\\n-\\n42\\ntotal\\n93\\n34\\n30\\nDocBank (DB)\\nFigure\\n77\\n71\\n31\\nTable\\n19\\n65\\n22\\ntotal\\n48\\n68\\n27\\nDocLayNet (DLN)\\nFigure\\n67\\n51\\n72\\nSec-header\\n53\\n-\\n68\\nTable\\n87\\n43\\n82\\nText\\n77\\n-\\n84\\ntotal\\n59\\n47\\n78\\nSection-header, Table and Text. Before training, we either mapped\\nor excluded DocLayNet’s other labels as specified in table 3, and\\nalso PubLayNet’s List to Text. Note that the different clustering of\\nlists (by list-element vs. whole list objects) naturally decreases the\\nmAP score for Text.\\nFor comparison of DocBank with DocLayNet, we trained only\\non Picture and Table clusters of each dataset. We had to exclude Text\\nbecause successive paragraphs are often grouped together into a\\nsingle object in DocBank. This paragraph grouping is incompatible\\nwith the individual paragraphs of DocLayNet. As can be seen in\\nTable 5, DocLayNet trained models yield better performance com-\\npared to the previous datasets. It is noteworthy that the models\\ntrained on PubLayNet and DocBank perform very well on their\\nown test set, but have a much lower performance on the foreign\\ndatasets. While this also applies to DocLayNet, the difference is\\nfar less pronounced. Thus we conclude that DocLayNet trained\\nmodels are overall more robust and will produce better results for\\nchallenging, unseen layouts.\\nExample Predictions\\nTo conclude this section, we illustrate the quality of layout predic-\\ntions one can expect from DocLayNet-trained models by providing\\na selection of examples without any further post-processing ap-\\nplied. Figure 6 shows selected layout predictions on pages from the\\ntest-set of DocLayNet. Results look decent in general across docu-\\nment categories, however one can also observe mistakes such as\\noverlapping clusters of different classes, or entirely missing boxes\\ndue to low confidence.\\n6\\nCONCLUSION\\nIn this paper, we presented the DocLayNet dataset. It provides the\\ndocument conversion and layout analysis research community a\\nnew and challenging dataset to improve and fine-tune novel ML\\nmethods on. In contrast to many other datasets, DocLayNet was\\ncreated by human annotation in order to obtain reliable layout\\nground-truth on a wide variety of publication- and typesetting-\\nstyles. Including a large proportion of documents outside the scien-\\ntific publishing domain adds significant value in this respect.\\nFrom the dataset, we have derived on the one hand reference\\nmetrics for human performance on document-layout annotation\\n(through double and triple annotations) and on the other hand eval-\\nuated the baseline performance of commonly used object detection\\nmethods. We also illustrated the impact of various dataset-related\\naspects on model performance through data-ablation experiments,\\nboth from a size and class-label perspective. Last but not least, we\\ncompared the accuracy of models trained on other public datasets\\nand showed that DocLayNet trained models are more robust.\\nTo date, there is still a significant gap between human and ML\\naccuracy on the layout interpretation task, and we hope that this\\nwork will inspire the research community to close that gap.\\nREFERENCES\\n[1] Max Göbel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table\\ncompetition. In 2013 12th International Conference on Document Analysis and\\nRecognition, pages 1449–1453, 2013.\\n[2] Christian Clausner, Apostolos Antonacopoulos, and Stefan Pletschacher. Ic-\\ndar2017 competition on recognition of documents with complex layouts -\\nrdcl2017. In 2017 14th IAPR International Conference on Document Analysis and\\nRecognition (ICDAR), volume 01, pages 1404–1410, 2017.\\n[3] Hervé Déjean, Jean-Luc Meunier, Liangcai Gao, Yilun Huang, Yu Fang, Florian\\nKleber, and Eva-Maria Lang. ICDAR 2019 Competition on Table Detection and\\nRecognition (cTDaR), April 2019. http://sac.founderit.com/.\\n[4] Antonio Jimeno Yepes, Peter Zhong, and Douglas Burdick. Competition on\\nscientific literature parsing. In Proceedings of the International Conference on\\nDocument Analysis and Recognition, ICDAR, pages 605–617. LNCS 12824, Springer-\\nVerlag, sep 2021.\\n[5] Logan Markewich, Hao Zhang, Yubin Xing, Navid Lambert-Shirzad, Jiang Zhexin,\\nRoy Lee, Zhi Li, and Seok-Bum Ko. Segmentation for document layout analysis:\\nnot dead yet. International Journal on Document Analysis and Recognition (IJDAR),\\npages 1–11, 01 2022.\\n[6] Xu Zhong, Jianbin Tang, and Antonio Jimeno-Yepes. Publaynet: Largest dataset\\never for document layout analysis. In Proceedings of the International Conference\\non Document Analysis and Recognition, ICDAR, pages 1015–1022, sep 2019.\\n[7] Minghao Li, Yiheng Xu, Lei Cui, Shaohan Huang, Furu Wei, Zhoujun Li, and\\nMing Zhou. Docbank: A benchmark dataset for document layout analysis. In\\nProceedings of the 28th International Conference on Computational Linguistics,\\nCOLING, pages 949–960. International Committee on Computational Linguistics,\\ndec 2020.\\n[8] Riaz Ahmad, Muhammad Tanvir Afzal, and M. Qadir. Information extraction\\nfrom pdf sources based on rule-based system using integrated formats. In SemWe-\\nbEval@ESWC, 2016.\\n[9] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature\\nhierarchies for accurate object detection and semantic segmentation. In IEEE\\nConference on Computer Vision and Pattern Recognition, CVPR, pages 580–587.\\nIEEE Computer Society, jun 2014.\\n[10] Ross B. Girshick. Fast R-CNN. In 2015 IEEE International Conference on Computer\\nVision, ICCV, pages 1440–1448. IEEE Computer Society, dec 2015.\\n[11] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards\\nreal-time object detection with region proposal networks. IEEE Transactions on\\nPattern Analysis and Machine Intelligence, 39(6):1137–1149, 2017.\\n[12] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross B. Girshick. Mask R-CNN.\\nIn IEEE International Conference on Computer Vision, ICCV, pages 2980–2988.\\nIEEE Computer Society, Oct 2017.\\n[13] Glenn Jocher, Alex Stoken, Ayush Chaurasia, Jirka Borovec, NanoCode012,\\nTaoXie, Yonghye Kwon, Kalen Michael, Liu Changyu, Jiacong Fang, Abhiram V,\\nLaughing, tkianai, yxNONG, Piotr Skalski, Adam Hogan, Jebastin Nadar, imyhxy,\\nLorenzo Mammana, Alex Wang, Cristi Fati, Diego Montes, Jan Hajek, Laurentiu\\n'),\n",
       " Document(metadata={'source': 'https://arxiv.org/pdf/2206.01062', 'file_path': 'https://arxiv.org/pdf/2206.01062', 'page': 8, 'total_pages': 9, 'format': 'PDF 1.5', 'title': 'DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis', 'author': 'Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed S. Nassar, and Peter Staar', 'subject': '-  Information systems  ->  Document structure; -  Applied computing  ->  Document analysis; -  Computing methodologies  ->  Machine learning; Computer vision; Object detection; ', 'keywords': 'PDF document conversion, layout segmentation, object-detection, data set, Machine Learning', 'creator': 'LaTeX with acmart 2018/04/14 v1.53 Typesetting articles for the Association for Computing Machinery and hyperref 2020-05-15 v7.00e Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.21', 'creationDate': 'D:20220603003241Z', 'modDate': 'D:20220603003241Z', 'trapped': ''}, page_content='DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\\nKDD ’22, August 14–18, 2022, Washington, DC, USA\\n4bed2a8aa51ac37058e79605821bbc426d032b0b6ca8bdf3409ed8508ccd8c67\\n2f2a06d08f5ad565d0f5e815f4ddf666365b2cff435cdaeb8850217e8a8efabf\\n7f2fd7293e04bf4f1756ae51f5779764933da1d1d2002e3915356050570fc75b\\n1b81cf65f47456ad4faa725d1eb09879bd633af16cfe2bf8cea661b87907bfac\\nb60da9d26f488cb133e47d101d35fda1bdca2671ade60764d1cd569590270327\\n2b7b8355a42ebef0cf91583aad9f30f7c9fa63c5b05911730ba15275c024965b\\nA\\nB\\nC\\nD\\nE\\nF\\nText\\nCaption\\nList-Item\\nFormula\\nTable\\nSection-Header\\nPicture\\nPage-Header\\nPage-Footer\\nTitle\\nFigure 6: Example layout predictions on selected pages from the DocLayNet test-set. (A, D) exhibit favourable results on\\ncoloured backgrounds. (B, C) show accurate list-item and paragraph differentiation despite densely-spaced lines. (E) demon-\\nstrates good table and figure distinction. (F) shows predictions on a Chinese patent with multiple overlaps, label confusion\\nand missing boxes.\\nDiaconu, Mai Thanh Minh, Marc, albinxavi, fatih, oleg, and wanghao yang. ul-\\ntralytics/yolov5: v6.0 - yolov5n nano models, roboflow integration, tensorflow\\nexport, opencv dnn support, October 2021.\\n[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander\\nKirillov, and Sergey Zagoruyko. End-to-end object detection with transformers.\\nCoRR, abs/2005.12872, 2020.\\n[15] Mingxing Tan, Ruoming Pang, and Quoc V. Le. Efficientdet: Scalable and efficient\\nobject detection. CoRR, abs/1911.09070, 2019.\\n[16] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Gir-\\nshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence\\nZitnick. Microsoft COCO: common objects in context, 2014.\\n[17] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick.\\nDetectron2, 2019.\\n[18] Nikolaos Livathinos, Cesar Berrospi, Maksym Lysak, Viktor Kuropiatnyk, Ahmed\\nNassar, Andre Carvalho, Michele Dolfi, Christoph Auer, Kasper Dinkla, and Peter\\nW. J. Staar. Robust pdf document conversion using recurrent neural networks. In\\nProceedings of the 35th Conference on Artificial Intelligence, AAAI, pages 15137–\\n15145, feb 2021.\\n[19] Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and Ming Zhou.\\nLayoutlm: Pre-training of text and layout for document image understanding.\\nIn Proceedings of the 26th ACM SIGKDD International Conference on Knowledge\\nDiscovery and Data Mining, KDD, pages 1192–1200, New York, USA, 2020. Asso-\\nciation for Computing Machinery.\\n[20] Shoubin Li, Xuyan Ma, Shuaiqun Pan, Jun Hu, Lin Shi, and Qing Wang. Vtlayout:\\nFusion of visual and text features for document layout analysis, 2021.\\n[21] Peng Zhang, Can Li, Liang Qiao, Zhanzhan Cheng, Shiliang Pu, Yi Niu, and Fei\\nWu. Vsr: A unified framework for document layout analysis combining vision,\\nsemantics and relations, 2021.\\n[22] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. Corpus\\nconversion service: A machine learning platform to ingest documents at scale.\\nIn Proceedings of the 24th ACM SIGKDD International Conference on Knowledge\\nDiscovery and Data Mining, KDD, pages 774–782. ACM, 2018.\\n[23] Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmenta-\\ntion for deep learning. Journal of Big Data, 6(1):60, 2019.\\n')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = str(documents[0].metadata.get(\"title\"))\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d3b71fb02d43f49b62cf7661e37a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding nodes:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filename and doc_id are the same for all nodes.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb1466386924dfa8a0c6930822dce1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Impact of Document Split', 'DocLayNet', 'Model performance', 'Random page-wise splitting', 'Dataset Comparison']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet dataset', 'Prediction Performance', 'Mask R-CNN R50 network', 'Document layout analysis', 'Human annotation']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet dataset', 'Prediction Performance', 'Mask R-CNN R50 network', 'Document layout analysis', 'Human annotation']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet dataset', 'Document-layout analysis', 'Mask R-CNN R50 network', 'Class label sets', 'Data ablation study']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['KDD 2022', 'DocLayNet dataset', 'Inter-annotator agreement', 'Corpus Conversion Service', 'Annotation process']\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet dataset', 'Document-layout analysis', 'Human-annotated dataset', 'Document categories', 'Technical content documents']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How does the impact of document split in train and test sets affect model performance in DocLayNet?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Corpus conversion service', 'Machine learning platform', 'Ingest documents at scale', 'ACM SIGKDD International Conference', 'Image data augmentation for deep learning']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What was the focus of the paper presented at the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What was the purpose of the KDD 2022 event held in Washington, DC, USA?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Corpus conversion service', 'Machine learning platform', 'Ingest documents at scale', 'ACM SIGKDD International Conference', 'Image data augmentation for deep learning']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What was the focus of the paper presented at the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the significance of the DocLayNet dataset in document-layout analysis?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Impact of Document Split', 'DocLayNet', 'Model performance', 'Random page-wise splitting', 'Dataset Comparison']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How does random page-wise splitting affect model performance in DocLayNet?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What advancements does the DocLayNet dataset bring to the field of document layout analysis?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What contributions does the DocLayNet dataset make to the field of document layout analysis?\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What characteristics make technical content documents challenging to analyze with high accuracy?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of the DocLayNet dataset in the context of document-layout analysis. It is clear in specifying the dataset (DocLayNet) and the field of interest (document-layout analysis), making the intent straightforward. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What is the significance of the DocLayNet dataset in document-layout analysis?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 2, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['ICDAR 2019 Competition on Table Detection and Recognition', 'Scientific literature parsing', 'Document layout analysis', 'Publaynet dataset', 'Docbank benchmark dataset']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the ICDAR 2019 Competition on Table Detection and Recognition?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the impact of DocLayNet's data size on Mask R-CNN's mAP score. It is clear in specifying the dataset (DocLayNet), the model (Mask R-CNN), and the metric of interest (mAP score). The intent is straightforward, seeking to understand the relationship between data size and model performance. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"How does DocLayNet's data size impact Mask R-CNN's mAP score?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks about the significance of the DocLayNet dataset in document-layout analysis, which is a broad inquiry into its importance and applications. The second question is specific to how the size of DocLayNet affects the performance of Mask R-CNN in terms of mAP, which is a more focused and technical inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for information about the ICDAR 2019 Competition on Table Detection and Recognition. It is clear in specifying the event (ICDAR 2019) and the specific competition (Table Detection and Recognition), making the intent straightforward. The question is independent and does not rely on external references or prior knowledge beyond what is stated. Therefore, it is understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What is the ICDAR 2019 Competition on Table Detection and Recognition?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the impact of random page-wise splitting on model performance in DocLayNet. It is clear in specifying the topic of interest (random page-wise splitting) and the context (model performance in DocLayNet). The intent is straightforward, seeking information on the effects of a specific technique within a defined framework. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Random page-wise splitting of DocLayNet can easily lead to accidental overestimation of model performance, as it gains approximately 10% in mAP over document-wise splitting.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Impact of Document Split', 'DocLayNet', 'Model performance', 'Random page-wise splitting', 'Dataset Comparison']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How does DocLayNet's wider variety of document layouts contribute to more robust layout detection models?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the characteristics that make technical content documents challenging to analyze with high accuracy. It is clear in its intent, seeking specific information about the difficulties associated with analyzing technical documents. The question is self-contained and does not rely on external references or prior knowledge beyond a general understanding of technical content analysis. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the contributions of the DocLayNet dataset to the field of document layout analysis. It is specific, independent, and has a clear intent, making it understandable and answerable based on the details provided. The question does not rely on external references or prior knowledge not included within the question itself, and it clearly seeks information about the impact or advancements brought by the DocLayNet dataset in its respective field.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What contributions does the DocLayNet dataset make to the field of document layout analysis?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Focusing', 'Changing the bulb', 'Zoom magnification', 'Diopter ring adjustment', 'BARSKA Model AY11236', 'Fixed power compound microscope']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How should you focus when using the zoom magnification on the BARSKA Model AY11236 microscope?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking for the purpose of the KDD 2022 event held in Washington, DC, USA. It does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What was the purpose of the KDD 2022 event held in Washington, DC, USA?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks how DocLayNet's wider variety of document layouts contributes to more robust layout detection models. It is clear in specifying the subject (DocLayNet) and the aspect of interest (contribution to robust layout detection models). The intent is to understand the impact of diverse document layouts on model robustness. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"DocLayNet's wider variety of document layouts leads to more robust layout detection models, as evidenced by the comparison in Table 5 where models trained on DocLayNet were evaluated on the test sets of other datasets.\", 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet', 'Human-Annotated Dataset', 'Document-Layout Analysis', 'KDD 2022', 'Example layout predictions']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the significance of DocLayNet in the field of document-layout analysis?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for guidance on how to focus when using the zoom magnification on the BARSKA Model AY11236 microscope. It is specific, independent, and has a clear intent, making it understandable and answerable based on the details provided. The question does not rely on external references or additional context, and it clearly seeks instructions on the focusing process for a particular microscope model.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: How should you focus when using the zoom magnification on the BARSKA Model AY11236 microscope?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the steps to ensure a clear image on the BARSKA Model AY11236, starting with the lowest magnification. It is specific, independent, and has a clear intent, making it understandable and answerable based on the details provided. The question does not rely on external references or prior knowledge not included within the question itself.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"Starting with the lowest magnification, what steps ensure a clear image on the BARSKA Model AY11236?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions are about achieving a clear image using the BARSKA Model AY11236 microscope, focusing on the use of zoom magnification and starting with low magnification, respectively. They share the same constraints and depth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] evolution_filter failed, retrying with 1\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the focus of a specific paper presented at the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. It is clear in specifying the event and the type of information sought (the focus of the paper). However, it does not provide the title or any identifying details of the paper, making it difficult to answer without additional context or access to the conference proceedings. To improve clarity and answerability, the question could include the title or authors of the paper, or specify a particular session or topic within the conference.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: What was the focus of the paper presented at the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for the focus of a specific paper presented at the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. It is clear in specifying the event and the type of information sought (the focus of the paper). However, it does not provide any details about the paper itself, such as the title, authors, or any specific session, making it difficult to identify which paper is being referred to. To improve clarity and answerability, the question could include more specific details about the paper, such as its title or authors, or specify if it is asking about a keynote or a particular session's paper.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet dataset', 'Object detection models', 'Mean average precision (mAP)', 'Mask R-CNN', 'YOLOv5']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What challenge does the DocLayNet dataset pose for the research community in terms of object detection model performance?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the impact of document split in train and test sets on model performance in DocLayNet. It is clear in specifying the topic of interest (document split, train and test sets, model performance, DocLayNet) and seeks detailed information on the relationship between these factors. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How does the impact of document split in train and test sets affect model performance in DocLayNet?\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 2, 'structure': 2, 'relevance': 3, 'score': 2.25}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Focusing', 'Changing the bulb', 'Zoom magnification', 'Diopter ring adjustment', 'BARSKA Model AY11236', 'Fixed power compound microscope']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are the primary uses of the BARSKA Model AY11236 fixed power compound microscope?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the advancements brought by the DocLayNet dataset to the field of document layout analysis. It is clear in specifying the dataset (DocLayNet) and the field of interest (document layout analysis), making the intent straightforward. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the significance of DocLayNet in the field of document-layout analysis. It is clear in specifying the topic of interest (DocLayNet) and the field (document-layout analysis), making the intent straightforward. The question does not rely on external references or unspecified contexts, making it self-contained and understandable. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the primary uses of the BARSKA Model AY11236 fixed power compound microscope. It is specific, independent, and has a clear intent, making it understandable and answerable based on the details provided. The question does not rely on external references or additional context, and it clearly seeks information about the applications of a particular microscope model.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What are the primary uses of the BARSKA Model AY11236 fixed power compound microscope?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the type of microscope used in biological, bacterial, and clinical studies. It is clear in specifying the context (types of studies) and seeks specific information (the type of microscope). The question is self-contained and does not rely on external references or additional context, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"Which microscope is used for biological, bacterial, and clinical studies?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks specifically about the uses of a particular microscope model, while the second question is seeking a recommendation for a type of microscope for specific studies. They have different constraints and requirements.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'BARSKA Model AY11236 is a powerful fixed power compound microscope designed for biological studies such as specimen examination. It can also be used for examining bacteria and for general clinical and medical studies and other scientific uses.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 0 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Corpus conversion service', 'Machine learning platform', 'Ingest documents at scale', 'ACM SIGKDD International Conference', 'Image data augmentation for deep learning']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What was the focus of the paper presented at the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for the focus of a specific paper presented at the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. It is clear in specifying the event and the type of information sought (the focus of the paper). However, it does not provide any details about the paper itself, such as the title, authors, or any specific session, making it difficult to identify which paper is being referred to. To improve clarity and answerability, the question could include more specific details about the paper, such as its title or authors, or specify if it is asking about a keynote or a particular session's paper.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: What was the focus of the paper presented at the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the focus of a specific paper presented at the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. It is clear in specifying the event and the type of information sought (the focus of the paper). However, it does not provide any details about the paper itself, such as the title, authors, or any specific session, making it difficult to identify which paper is being referred to. To improve clarity and answerability, the question could include additional identifying information about the paper, such as the title or authors, or specify a particular session or topic within the conference.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet dataset', 'Document-layout analysis', 'Deep learning methods', 'Object detection models', 'Annotation instances']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How many total annotation instances are provided in the DocLayNet dataset?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking for the total number of annotation instances in the DocLayNet dataset. It does not rely on external references or additional context, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The DocLayNet dataset provides 91104 total annotation instances.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['KDD 2022', 'DocLayNet dataset', 'Inter-annotator agreement', 'Corpus Conversion Service', 'Annotation process']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What information is provided in the overview of the DocLayNet dataset?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for information provided in the overview of the DocLayNet dataset. It is clear in specifying the dataset of interest (DocLayNet) and seeks information about its overview. The intent is straightforward, and the question is self-contained, not relying on external references or prior knowledge beyond the dataset's name. Therefore, it meets the criteria for clarity and answerability.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What information is provided in the overview of the DocLayNet dataset?\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The mAP score rises sharply in the beginning and eventually levels out as the data size increases. The curve flattens out between the 80% and 100% mark, indicating that the model would not improve significantly by further increasing the data size.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['KDD 2022', 'DocLayNet test set', 'Object detection networks', 'Mask R-CNN', 'Faster R-CNN', 'YOLOv5x6', 'ResNet-50', 'ResNet-101', 'COCO 2017 dataset', 'Document-layout annotation']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What was the focus of the experiments presented at KDD 2022 regarding document-layout analysis?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the focus of experiments presented at KDD 2022 related to document-layout analysis. It is specific in terms of the event (KDD 2022) and the topic (document-layout analysis), making the intent clear. The question does not rely on external references or unspecified contexts, making it independent and self-contained. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What was the focus of the experiments presented at KDD 2022 regarding document-layout analysis?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: How did the performance of object detection models on the DocLayNet dataset vary with different class label sets and training data sizes?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question is clear and specific, asking about the challenges posed by the DocLayNet dataset for the research community in terms of object detection model performance. It does not rely on external references or unspecified contexts, making it self-contained and understandable. The intent is also clear, seeking information on the specific challenges related to object detection models when using the DocLayNet dataset.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What challenge does the DocLayNet dataset pose for the research community in terms of object detection model performance?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: How does the performance of YOLOv5x6 on specific labels in the DocLayNet dataset highlight the challenge of achieving human-level accuracy in object detection models, considering the baseline mAP scores and annotation methods used?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the performance variation of object detection models on the DocLayNet dataset with respect to different class label sets and training data sizes. It is specific in terms of the dataset (DocLayNet), the type of models (object detection), and the variables of interest (class label sets and training data sizes). The intent is clear, seeking an analysis of performance variation. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How did obj. detection models perform on DocLayNet with varying class labels and training sizes?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks about the general focus of experiments at KDD 2022 related to document-layout analysis, while the second question is specific to the performance of object detection models on DocLayNet with varying class labels and training sizes. These questions differ in both depth and breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the performance of YOLOv5x6 on specific labels in the DocLayNet dataset and how it highlights the challenge of achieving human-level accuracy in object detection models. It also references baseline mAP scores and annotation methods. While the intent is clear in seeking an analysis of YOLOv5x6's performance and its implications, the question assumes familiarity with the DocLayNet dataset, specific labels, baseline mAP scores, and annotation methods without providing details. To improve clarity and answerability, the question could specify which labels are of interest, provide a brief overview of the baseline mAP scores and annotation methods, or frame the question in a way that does not rely on this external information.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How does the performance of YOLOv5x6 on specific labels in the DocLayNet dataset highlight the challenge of achieving human-level accuracy in object detection models, considering the baseline mAP scores and annotation methods used?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the performance of YOLOv5x6 on specific labels in the DocLayNet dataset, focusing on the challenge of achieving human-level accuracy in object detection models. It mentions baseline mAP scores and annotation methods but does not provide these details within the question itself. This makes the question dependent on external references and unclear for those without access to the specific dataset or baseline scores. To improve clarity and answerability, the question could include a brief description of the baseline mAP scores and annotation methods or frame the question in a way that does not rely on these external details.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 2, 'structure': 1, 'relevance': 2, 'score': 1.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Optical paths', 'Transmitted illumination', 'Magnification from 40x to 1000x', 'Coarse and fine focus adjustments', 'Rotating head', 'Circling Minimums', 'Expanded circling areas', 'Airport sketch', 'Runway dimensions', 'Taxiways and aprons']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the purpose of the rotating head in the instrument described?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the purpose of the rotating head in an unspecified instrument. While it is clear that the question seeks to understand the function of the rotating head, it lacks sufficient context about the instrument being referred to. Without knowing what the instrument is, it is impossible to provide a specific and accurate answer. To improve clarity and answerability, the question should specify the instrument in question or provide a brief description of it.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: What is the purpose of the rotating head in the instrument described?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the purpose of the rotating head in an unspecified instrument. While it is clear that the question seeks to understand the function of the rotating head, it lacks sufficient context about the instrument being referred to. Without knowing what the instrument is, it is impossible to provide a specific and accurate answer. To improve clarity and answerability, the question should specify the instrument or provide a brief description of it.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['KDD 2022', 'DocLayNet dataset', 'Inter-annotator agreement', 'Corpus Conversion Service', 'Annotation process']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What information is provided in the overview of the DocLayNet dataset?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for information provided in the overview of the DocLayNet dataset. It is clear in specifying the dataset of interest (DocLayNet) and seeks information about its overview. The intent is straightforward, and the question is self-contained, not relying on external references or prior knowledge beyond the dataset's name. Therefore, it meets the criteria for clarity and answerability.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What information is provided in the overview of the DocLayNet dataset?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: How does the method of splitting DocLayNet into train and test sets influence the mAP scores of a Mask R-CNN R50 model, considering the impact of class label variations?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the influence of the method of splitting DocLayNet into train and test sets on the mAP scores of a Mask R-CNN R50 model, with a specific focus on the impact of class label variations. It is clear in specifying the dataset (DocLayNet), the model (Mask R-CNN R50), and the metric of interest (mAP scores). The intent is also clear, seeking to understand the relationship between the data split method and model performance, considering class label variations. This makes the question specific, independent, and clear in its intent.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How does splitting DocLayNet affect Mask R-CNN R50 mAP scores with class label variations?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the general impact of document split on model performance in DocLayNet, while the second question specifically addresses the effect of splitting on Mask R-CNN R50 mAP scores with class label variations. These questions differ in both depth and breadth of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Splitting DocLayNet affects Mask R-CNN R50 mAP scores significantly. A naive page-wise split results in approximately a 10% point improvement in mAP scores compared to document-wise splitting. This is evident from the performance differences shown in Table 4, where page-wise splitting consistently yields higher mAP scores across various class label sets.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: How does the ICDAR 2019 cTDaR competition relate to advancements in document layout analysis datasets like DocLayNet?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the relationship between the ICDAR 2019 cTDaR competition and advancements in document layout analysis datasets like DocLayNet. It is clear in specifying the competition (ICDAR 2019 cTDaR) and the dataset (DocLayNet), and it seeks information on their relationship or impact on each other. The intent is clear, and the question is self-contained, not relying on external references or unspecified contexts. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How is the ICDAR 2019 cTDaR comp linked to DocLayNet advancements?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks for a description of the ICDAR 2019 Competition on Table Detection and Recognition, while the second question asks about the relationship between the ICDAR 2019 cTDaR competition and advancements in DocLayNet. These questions have different constraints and requirements, as well as different depths and breadths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['KDD 2022', 'DocLayNet test set', 'Object detection networks', 'Mask R-CNN', 'Faster R-CNN', 'YOLOv5x6', 'ResNet-50', 'ResNet-101', 'COCO 2017 dataset', 'Document-layout annotation']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the prediction performance (mAP@0.5-0.95) of different object detection networks on the DocLayNet test set?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the prediction performance (mAP@0.5-0.95) of different object detection networks on the DocLayNet test set. It is clear in specifying the metric of interest (mAP@0.5-0.95), the task (object detection), and the dataset (DocLayNet test set). The intent is unambiguous, seeking a comparison of performance metrics across different networks. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The prediction performance (mAP@0.5-0.95) of different object detection networks on the DocLayNet test set is as follows: MRCNN (R50: 72.4, R101: 73.5), FRCNN (R101: 73.4), YOLO (v5x6: 76.8).', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the focus of a specific paper presented at the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. It is clear in specifying the event and the type of information sought (the focus of the paper). However, it does not provide the title or any identifying details of the paper, making it difficult to answer without additional context or access to the conference proceedings. To improve clarity and answerability, the question could include the title or authors of the paper, or specify a particular session or topic within the conference.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: What was the focus of the paper presented at the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the focus of a specific paper presented at the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. It is clear in specifying the event and the type of information sought (the focus of the paper). However, it does not provide any details about the paper itself, such as the title, authors, or any specific session, making it difficult to identify which paper is being referred to. To improve clarity and answerability, the question could include more specific information about the paper, such as its title or authors, or specify that it is asking about the general focus of papers presented at the conference if that is the intent.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['KDD 2022', 'DocLayNet test set', 'Object detection networks', 'Mask R-CNN', 'Faster R-CNN', 'YOLOv5x6', 'ResNet-50', 'ResNet-101', 'COCO 2017 dataset', 'Document-layout annotation']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What measures were implemented to ensure clear and unbiased baseline numbers for human document-layout annotation?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the measures implemented to ensure clear and unbiased baseline numbers for human document-layout annotation. It is specific and clear in its intent, seeking information on the steps or strategies used to achieve clarity and lack of bias in baseline numbers. The question is self-contained and does not rely on external references or prior knowledge, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'To ensure clear and unbiased baseline numbers for human document-layout annotation, the following measures were implemented: 1) Snapping boxes around text segments to obtain pixel-accurate annotation and reduce time and effort. 2) Automatically shrinking every user-drawn box to the minimum bounding-box around the enclosed text-cells for all purely text-based segments, excluding Table and Picture. 3) Instructing annotation staff to minimize inclusion of surrounding whitespace while including all graphical lines for Table and Picture. 4) Establishing a way to flag pages as rejected for cases where no valid annotation according to the label guidelines could be achieved, such as PDF pages that render incorrectly or contain layouts that are impossible to capture with non-overlapping rectangles.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Technical content documents are challenging to analyze with high accuracy due to their complex tables, figures, plots, and captions, which result in challenging layouts.', 'verdict': 1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 1, 'depth': 1, 'structure': 1, 'relevance': 1, 'score': 1.0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet dataset', 'Prediction Performance', 'Mask R-CNN R50 network', 'Document layout analysis', 'Human annotation']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What is the prediction performance of the Mask R-CNN R50 network across the PubLayNet, DocBank, and DocLayNet datasets?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the prediction performance of the Mask R-CNN R50 network across three specific datasets: PubLayNet, DocBank, and DocLayNet. It is clear in specifying the model (Mask R-CNN R50) and the datasets of interest, making the intent straightforward and unambiguous. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What is the prediction performance of the Mask R-CNN R50 network across the PubLayNet, DocBank, and DocLayNet datasets?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for a comparison of DocLayNet's mean Average Precision (mAP) with that of PubLayNet and DocBank. It is clear in specifying the metric of interest (mAP) and the datasets/models for comparison (DocLayNet, PubLayNet, DocBank). The intent is straightforward, seeking a comparative analysis of performance. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"How does DocLayNet's mAP compare to PubLayNet and DocBank?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks for the prediction performance of a specific network across three datasets, requiring detailed performance metrics. The second question focuses on comparing the mAP (mean Average Precision) of one dataset to two others, which is a narrower inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'DocLayNet trained models yield better performance compared to the previous datasets. It is noteworthy that the models trained on PubLayNet and DocBank perform very well on their own test set, but have a much lower performance on the foreign datasets. While this also applies to DocLayNet, the difference is far less pronounced. Thus we conclude that DocLayNet trained models are overall more robust and will produce better results for challenging, unseen layouts.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The answer to given question is not present in context', 'verdict': -1}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet dataset', 'Document-layout analysis', 'Annotation guideline', 'Annotation consistency', 'Training and production phases']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How did the introduction of the annotation guideline impact annotation consistency?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the impact of the introduction of an annotation guideline on annotation consistency. It is clear in its intent, seeking information on the effect of a specific intervention (the annotation guideline) on a particular outcome (annotation consistency). The question is independent and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How did the introduction of the annotation guideline impact annotation consistency?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: How does the distribution of document categories and the inter-annotator agreement metrics reflect the complexity and variability of the DocLayNet dataset?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the distribution of document categories and inter-annotator agreement metrics in relation to the complexity and variability of the DocLayNet dataset. It is clear in specifying the dataset (DocLayNet) and the metrics of interest (distribution of document categories, inter-annotator agreement). However, it assumes familiarity with the DocLayNet dataset and does not provide any context or definitions for the terms used. To improve clarity and answerability, the question could include a brief description of the DocLayNet dataset and define what is meant by 'complexity' and 'variability' in this context.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How does the distribution of document categories and the inter-annotator agreement metrics reflect the complexity and variability of the DocLayNet dataset?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks about the distribution of document categories and inter-annotator agreement metrics in relation to the complexity and variability of the DocLayNet dataset. It is clear in its intent, specifying the aspects of interest (distribution of categories, inter-annotator agreement) and the dataset in question (DocLayNet). However, it assumes familiarity with the DocLayNet dataset and the specific metrics without providing additional context or definitions. To improve clarity and answerability, the question could briefly describe what the DocLayNet dataset is and what is meant by 'inter-annotator agreement metrics'.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do doc categories and inter-annotator metrics show DocLayNet's complexity?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks for a general overview of the DocLayNet dataset, while the second question specifically inquires about document categories and inter-annotator metrics, indicating different depths and focuses of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: How did the introduction of the annotation guideline affect the consistency of annotations across the diverse document categories in DocLayNet?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the impact of introducing an annotation guideline on the consistency of annotations across diverse document categories in DocLayNet. It is specific in its focus (annotation guideline, consistency of annotations, diverse document categories, DocLayNet) and clear in its intent, seeking to understand the effect of a particular intervention. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How did the annotation guideline impact consistency in DocLayNet?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the impact of the annotation guideline on annotation consistency, sharing the same constraints, requirements, and depth of inquiry.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet dataset', 'Document-layout analysis', 'Deep learning methods', 'Object detection models', 'Annotation instances']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How have object detection models impacted the field of document-layout analysis?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the impact of object detection models on the field of document-layout analysis. It is clear in specifying the subject of interest (object detection models) and the area of application (document-layout analysis). The intent is to understand the influence or changes brought about by these models in this specific field. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How have object detection models impacted the field of document-layout analysis?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: How does the variability in document layouts and the use of human annotations in the DocLayNet dataset influence the performance of object detection models compared to automated datasets like PubLayNet and DocBank?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the influence of variability in document layouts and the use of human annotations in the DocLayNet dataset on the performance of object detection models, compared to automated datasets like PubLayNet and DocBank. It is clear in specifying the datasets (DocLayNet, PubLayNet, DocBank) and the factors of interest (variability in document layouts, human annotations). The intent is also clear, seeking a comparison of performance impacts. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do layout variability and human annotations in DocLayNet affect object detection vs. automated datasets like PubLayNet and DocBank?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question focuses on the overall impact of object detection models on document-layout analysis, while the second question specifically addresses the effects of layout variability and human annotations in DocLayNet compared to automated datasets like PubLayNet and DocBank. These questions have different constraints and requirements, as well as different depths and breadths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet dataset', 'Document-layout analysis', 'Human-annotated dataset', 'Document categories', 'Technical content documents']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What are the six distinct document categories included in DocLayNet?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the six distinct document categories included in DocLayNet. It is specific, independent, and has a clear intent, making it understandable and answerable based on the details provided. No additional context or external references are needed to comprehend or respond to this question.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] simple question generated: What are the six distinct document categories included in DocLayNet?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks which categories include Financial Reports and Government Tenders. It is clear in its intent, seeking specific categories that encompass these two items. The question is independent and does not rely on external references or additional context. Therefore, it is understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [ReasoningEvolution] question compressed: \"Which categories include Financial Reports and Government Tenders?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks for the six distinct document categories in DocLayNet, while the second question specifically asks about categories covering Financial Reports and Govt Tenders. They have different constraints and requirements.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The categories that cover Financial Reports and Government Tenders are Financial Reports and Government Tenders.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'DocLayNet includes diverse and complex layouts from a large variety of public sources and relies on human annotation instead of automation approaches to generate the dataset. This results in more robust layout recovery compared to models trained on automated datasets like PubLayNet and DocBank, which have limited variability due to their reliance on structured source data from scientific document repositories.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: What were the key document categories and annotation challenges addressed during the KDD 2022 event in Washington, DC?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for the key document categories and annotation challenges addressed during the KDD 2022 event in Washington, DC. It is specific in terms of the event (KDD 2022) and location (Washington, DC), and it clearly seeks information about document categories and annotation challenges. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What doc categories and annotation challenges were tackled at KDD 2022 in DC?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks about the purpose of the KDD 2022 event, while the second question focuses on specific document categories and annotation challenges addressed at the event. These questions have different constraints and requirements, as well as different depths and breadths of inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: How does the DocLayNet dataset's annotation consistency and model robustness compare to other datasets in document layout analysis?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for a comparison of the DocLayNet dataset's annotation consistency and model robustness against other datasets in the field of document layout analysis. It is clear in specifying the aspects of interest (annotation consistency and model robustness) and the domain (document layout analysis). However, it assumes familiarity with the DocLayNet dataset and other datasets in the same field without providing any context or examples. To improve clarity and answerability, the question could benefit from specifying which other datasets are being referred to or providing a brief description of the DocLayNet dataset and the criteria used for comparison.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How does the DocLayNet dataset's annotation consistency and model robustness compare to other datasets in document layout analysis?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks for a comparison of the DocLayNet dataset's annotation consistency and model robustness against other datasets in the field of document layout analysis. It is clear in specifying the aspects of interest (annotation consistency and model robustness) and the domain (document layout analysis). However, it assumes familiarity with the DocLayNet dataset and other datasets in the same field without providing any context or examples. To improve clarity and answerability, the question could benefit from specifying which other datasets are being considered for comparison or providing a brief description of the DocLayNet dataset and its relevance in the field.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 1 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['Impact of Document Split', 'DocLayNet', 'Model performance', 'Random page-wise splitting', 'Dataset Comparison']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How does the dataset comparison in the paper provide evidence that DocLayNet's wider variety of document layouts leads to more robust layout detection models?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks how the dataset comparison in a specific paper provides evidence that DocLayNet's wider variety of document layouts leads to more robust layout detection models. While it specifies the topic (DocLayNet and its document layouts) and the type of evidence sought (dataset comparison), it assumes access to and familiarity with the specific paper being referenced. This reliance on an external document makes the question unclear for those who do not have access to or knowledge of the paper. To improve clarity and answerability, the question could include a brief summary of the key findings or comparisons made in the paper, or it could be reframed to ask about the general principles or findings related to DocLayNet's layout variety and model robustness without referencing a specific document.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] rewritten question: How does the dataset comparison in the paper provide evidence that DocLayNet's wider variety of document layouts leads to more robust layout detection models?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks how the dataset comparison in a specific paper provides evidence that DocLayNet's wider variety of document layouts leads to more robust layout detection models. While it specifies the topic (DocLayNet and its document layouts) and the type of evidence sought (dataset comparison), it assumes access to and familiarity with the specific paper being referenced. This reliance on an external document makes the question unclear for those who do not have access to or knowledge of the paper. To improve clarity and answerability, the question could include a brief summary of the dataset comparison or the key findings from the paper, or alternatively, frame the question in a way that does not rely on specific, unpublished documents.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet dataset', 'Document-layout analysis', 'Human-annotated dataset', 'Document categories', 'Technical content documents']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: What challenges are associated with analyzing technical content documents in DocLayNet?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the challenges associated with analyzing technical content documents in DocLayNet. It is clear in specifying the subject (technical content documents) and the context (DocLayNet), making the intent straightforward. The question does not rely on external references or unspecified contexts, making it self-contained and understandable. Therefore, it meets the criteria for clarity and answerability.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: What challenges are associated with analyzing technical content documents in DocLayNet?\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Object detection models, specifically a Mask R-CNN R50 network, were evaluated on DocLayNet with different class label sets and training sizes. The performance in mAP@0.5-0.95 scores varied based on the number of class labels and the fraction of the dataset used for training. For example, with 11 class labels, the overall mAP score was 72, while with 5 class labels, it was 78. The learning curve showed that the mAP score increased sharply with the initial increase in training data size and eventually leveled out around the 80% mark, indicating that further increasing the data size would not significantly improve the model performance.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The DocLayNet dataset includes a variety of document categories such as Financial Reports, Manuals, Scientific Articles, Laws & Regulations, Patents, and Government Tenders. The inter-annotator agreement is measured using the mAP@0.5-0.95 metric, with accuracy ranges provided for each class label. These metrics, along with the diverse document categories, highlight the complexity of the dataset.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The document categories tackled at KDD 2022 in DC were Financial Reports, Manuals, Scientific Articles, Laws & Regulations, Patents, and Government Tenders. The annotation challenges included ensuring documents are free to use, excluding scanned documents, achieving a balance of pages between different document categories, and dealing with documents dense in complex tables, figures, plots, and captions.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: What factors contribute to the difficulty of achieving high-accuracy layout analysis in DocLayNet, considering the variability in document types and the methods used for annotation?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the factors contributing to the difficulty of achieving high-accuracy layout analysis in DocLayNet, considering the variability in document types and the methods used for annotation. It is specific in its focus on DocLayNet and the challenges related to layout analysis, and it clearly seeks information on contributing factors. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"What makes high-accuracy layout analysis in DocLayNet hard?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'Both questions inquire about the difficulties in analyzing technical content documents in DocLayNet, requiring similar depth and breadth of explanation.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.INFO] retrying evolution: 2 times\n",
      "[ragas.testset.filters.DEBUG] context scoring: {'clarity': 2, 'depth': 3, 'structure': 2, 'relevance': 3, 'score': 2.5}\n",
      "[ragas.testset.evolutions.DEBUG] keyphrases in merged node: ['DocLayNet dataset', 'Object detection models', 'Mean average precision (mAP)', 'Mask R-CNN', 'YOLOv5']\n",
      "[ragas.testset.evolutions.INFO] seed question generated: How do object detection models perform on the DocLayNet dataset according to the evaluation using mean average precision (mAP)?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks about the performance of object detection models on the DocLayNet dataset, specifically evaluated using mean average precision (mAP). It is clear in specifying the dataset (DocLayNet), the type of models (object detection), and the evaluation metric (mAP). This makes the question specific, independent, and with a clear intent, allowing for a direct and relevant response based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] simple question generated: How do object detection models perform on the DocLayNet dataset according to the evaluation using mean average precision (mAP)?\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: How do the mAP scores of Mask R-CNN, Faster R-CNN, and YOLOv5x6 compare across different document elements in the DocLayNet dataset, and what does this indicate about their performance relative to human annotations?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': 'The question asks for a comparison of mAP scores of Mask R-CNN, Faster R-CNN, and YOLOv5x6 across different document elements in the DocLayNet dataset and seeks an interpretation of their performance relative to human annotations. It is clear in specifying the models (Mask R-CNN, Faster R-CNN, YOLOv5x6), the dataset (DocLayNet), and the metric (mAP scores). The intent is also clear, as it seeks both a comparison and an interpretation of the results. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do Mask R-CNN, Faster R-CNN, and YOLOv5x6 mAP scores compare on DocLayNet, and what does this say about their performance vs. human annotations?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': 'The first question asks about the performance of object detection models on the DocLayNet dataset using mAP, while the second question specifically compares the mAP scores of Mask R-CNN, Faster R-CNN, and YOLOv5x6 on DocLayNet and also asks for an interpretation of their performance relative to human annotations. The second question has a broader scope and deeper inquiry.', 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'Mask R-CNN and Faster R-CNN produce very comparable mAP scores, indicating that pixel-based image segmentation derived from bounding-boxes does not help to obtain better predictions. On the other hand, the more recent Yolov5x model does very well and even out-performs humans on selected labels such as Text, Table and Picture. Overall, the variation in mAP between the models is rather low, but overall between 6 and 10% lower than the mAP computed from the pairwise human annotations on triple-annotated pages.', 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question generated: How does the distribution of document categories and the inter-annotator agreement metrics in the DocLayNet dataset inform the dataset's overall structure and annotation quality?\n",
      "[ragas.testset.filters.DEBUG] filtered question: {'feedback': \"The question asks how the distribution of document categories and inter-annotator agreement metrics in the DocLayNet dataset inform the dataset's overall structure and annotation quality. It is clear in specifying the dataset (DocLayNet) and the aspects of interest (distribution of document categories, inter-annotator agreement metrics). The intent is to understand the implications of these aspects on the dataset's structure and quality. The question is self-contained and does not rely on external references or unspecified contexts, making it understandable and answerable based on the details provided.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] [MultiContextEvolution] multicontext question compressed: \"How do DocLayNet's category distribution and inter-annotator metrics reflect its structure and quality?\"\n",
      "[ragas.testset.filters.DEBUG] evolution filter: {'reason': \"The first question asks for a general overview of the DocLayNet dataset, while the second question specifically inquires about the category distribution and inter-annotator metrics, which are more detailed aspects of the dataset's structure and quality.\", 'verdict': 0}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': \"DocLayNet's category distribution includes Financial Reports (32%), Manuals (21%), Scientific Articles (17%), Laws & Regulations (16%), Patents (8%), and Government Tenders (6%). The inter-annotator agreement is computed as the mAP@0.5-0.95 metric between pairwise annotations from the triple-annotated pages, with accuracy ranges provided for each class label.\", 'verdict': 1}\n",
      "[ragas.testset.evolutions.DEBUG] answer generated: {'answer': 'The DocLayNet dataset provides the document conversion and layout analysis research community a new and challenging dataset to improve and fine-tune novel ML methods on. It was created by human annotation to obtain reliable layout ground-truth on a wide variety of publication- and typesetting-styles. Including a large proportion of documents outside the scientific publishing domain adds significant value. The dataset also offers reference metrics for human performance on document-layout annotation and evaluates the baseline performance of commonly used object detection methods. Additionally, it shows that models trained on DocLayNet are more robust compared to those trained on other public datasets.', 'verdict': 1}\n"
     ]
    }
   ],
   "source": [
    "testset = generator.generate_with_langchain_docs(documents, 20, distributions, with_debugging_logs=True) # Default  RunConfig(max_retries=15, max_wait=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How does random page-wise splitting affect mod...</td>\n",
       "      <td>[-average improves\\nby around 5%, in particula...</td>\n",
       "      <td>Random page-wise splitting of DocLayNet can ea...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What advancements does the DocLayNet dataset b...</td>\n",
       "      <td>[KDD ’22, August 14–18, 2022, Washington, DC, ...</td>\n",
       "      <td>The DocLayNet dataset provides the document co...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the significance of DocLayNet in the f...</td>\n",
       "      <td>[DocLayNet: A Large Human-Annotated Dataset fo...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What characteristics make technical content do...</td>\n",
       "      <td>[DocLayNet: A Large Human-Annotated Dataset fo...</td>\n",
       "      <td>Technical content documents are challenging to...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the prediction performance (mAP@0.5-0....</td>\n",
       "      <td>[KDD ’22, August 14–18, 2022, Washington, DC, ...</td>\n",
       "      <td>The prediction performance (mAP@0.5-0.95) of d...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What measures were implemented to ensure clear...</td>\n",
       "      <td>[KDD ’22, August 14–18, 2022, Washington, DC, ...</td>\n",
       "      <td>To ensure clear and unbiased baseline numbers ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How does DocLayNet's wider variety of document...</td>\n",
       "      <td>[-average improves\\nby around 5%, in particula...</td>\n",
       "      <td>DocLayNet's wider variety of document layouts ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How many total annotation instances are provid...</td>\n",
       "      <td>[test-split. Last\\nbut not least, we compare t...</td>\n",
       "      <td>The DocLayNet dataset provides 91104 total ann...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What doc categories and annotation challenges ...</td>\n",
       "      <td>[KDD ’22, August 14–18, 2022, Washington, DC, ...</td>\n",
       "      <td>The document categories tackled at KDD 2022 in...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How do Mask R-CNN, Faster R-CNN, and YOLOv5x6 ...</td>\n",
       "      <td>[\\nsize of the DocLayNet dataset with similar ...</td>\n",
       "      <td>Mask R-CNN and Faster R-CNN produce very compa...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How do DocLayNet's category distribution and i...</td>\n",
       "      <td>[KDD ’22, August 14–18, 2022, Washington, DC, ...</td>\n",
       "      <td>DocLayNet's category distribution includes Fin...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>How did obj. detection models perform on DocLa...</td>\n",
       "      <td>[KDD ’22, August 14–18, 2022, Washington, DC, ...</td>\n",
       "      <td>Object detection models, specifically a Mask R...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>How does splitting DocLayNet affect Mask R-CNN...</td>\n",
       "      <td>[-average improves\\nby around 5%, in particula...</td>\n",
       "      <td>Splitting DocLayNet affects Mask R-CNN R50 mAP...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>How do doc categories and inter-annotator metr...</td>\n",
       "      <td>[KDD ’22, August 14–18, 2022, Washington, DC, ...</td>\n",
       "      <td>The DocLayNet dataset includes a variety of do...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How do layout variability and human annotation...</td>\n",
       "      <td>[test-split. Last\\nbut not least, we compare t...</td>\n",
       "      <td>DocLayNet includes diverse and complex layouts...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>How is the ICDAR 2019 cTDaR comp linked to Doc...</td>\n",
       "      <td>[ pages 1404–1410, 2017.\\n[3] Hervé Déjean, Je...</td>\n",
       "      <td>The answer to given question is not present in...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Which categories cover Financial Reports and G...</td>\n",
       "      <td>[DocLayNet: A Large Human-Annotated Dataset fo...</td>\n",
       "      <td>The categories that cover Financial Reports an...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Which microscope for bio, bacterial, and clini...</td>\n",
       "      <td>[ the   \\n    eyepiece. \\n \\nFOCUSING\\n1. Remo...</td>\n",
       "      <td>BARSKA Model AY11236 is a powerful fixed power...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>How does DocLayNet size affect Mask R-CNN's mAP?</td>\n",
       "      <td>[DocLayNet: A Large Human-Annotated Dataset fo...</td>\n",
       "      <td>The mAP score rises sharply in the beginning a...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>How does DocLayNet's mAP stack up to PubLayNet...</td>\n",
       "      <td>[KDD ’22, August 14–18, 2022, Washington, DC, ...</td>\n",
       "      <td>DocLayNet trained models yield better performa...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': 'https://arxiv.org/pdf/2206.01062'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   How does random page-wise splitting affect mod...   \n",
       "1   What advancements does the DocLayNet dataset b...   \n",
       "2   What is the significance of DocLayNet in the f...   \n",
       "3   What characteristics make technical content do...   \n",
       "4   What is the prediction performance (mAP@0.5-0....   \n",
       "5   What measures were implemented to ensure clear...   \n",
       "6   How does DocLayNet's wider variety of document...   \n",
       "7   How many total annotation instances are provid...   \n",
       "8   What doc categories and annotation challenges ...   \n",
       "9   How do Mask R-CNN, Faster R-CNN, and YOLOv5x6 ...   \n",
       "10  How do DocLayNet's category distribution and i...   \n",
       "11  How did obj. detection models perform on DocLa...   \n",
       "12  How does splitting DocLayNet affect Mask R-CNN...   \n",
       "13  How do doc categories and inter-annotator metr...   \n",
       "14  How do layout variability and human annotation...   \n",
       "15  How is the ICDAR 2019 cTDaR comp linked to Doc...   \n",
       "16  Which categories cover Financial Reports and G...   \n",
       "17  Which microscope for bio, bacterial, and clini...   \n",
       "18   How does DocLayNet size affect Mask R-CNN's mAP?   \n",
       "19  How does DocLayNet's mAP stack up to PubLayNet...   \n",
       "\n",
       "                                             contexts  \\\n",
       "0   [-average improves\\nby around 5%, in particula...   \n",
       "1   [KDD ’22, August 14–18, 2022, Washington, DC, ...   \n",
       "2   [DocLayNet: A Large Human-Annotated Dataset fo...   \n",
       "3   [DocLayNet: A Large Human-Annotated Dataset fo...   \n",
       "4   [KDD ’22, August 14–18, 2022, Washington, DC, ...   \n",
       "5   [KDD ’22, August 14–18, 2022, Washington, DC, ...   \n",
       "6   [-average improves\\nby around 5%, in particula...   \n",
       "7   [test-split. Last\\nbut not least, we compare t...   \n",
       "8   [KDD ’22, August 14–18, 2022, Washington, DC, ...   \n",
       "9   [\\nsize of the DocLayNet dataset with similar ...   \n",
       "10  [KDD ’22, August 14–18, 2022, Washington, DC, ...   \n",
       "11  [KDD ’22, August 14–18, 2022, Washington, DC, ...   \n",
       "12  [-average improves\\nby around 5%, in particula...   \n",
       "13  [KDD ’22, August 14–18, 2022, Washington, DC, ...   \n",
       "14  [test-split. Last\\nbut not least, we compare t...   \n",
       "15  [ pages 1404–1410, 2017.\\n[3] Hervé Déjean, Je...   \n",
       "16  [DocLayNet: A Large Human-Annotated Dataset fo...   \n",
       "17  [ the   \\n    eyepiece. \\n \\nFOCUSING\\n1. Remo...   \n",
       "18  [DocLayNet: A Large Human-Annotated Dataset fo...   \n",
       "19  [KDD ’22, August 14–18, 2022, Washington, DC, ...   \n",
       "\n",
       "                                         ground_truth evolution_type  \\\n",
       "0   Random page-wise splitting of DocLayNet can ea...         simple   \n",
       "1   The DocLayNet dataset provides the document co...         simple   \n",
       "2   The answer to given question is not present in...         simple   \n",
       "3   Technical content documents are challenging to...         simple   \n",
       "4   The prediction performance (mAP@0.5-0.95) of d...         simple   \n",
       "5   To ensure clear and unbiased baseline numbers ...         simple   \n",
       "6   DocLayNet's wider variety of document layouts ...         simple   \n",
       "7   The DocLayNet dataset provides 91104 total ann...         simple   \n",
       "8   The document categories tackled at KDD 2022 in...  multi_context   \n",
       "9   Mask R-CNN and Faster R-CNN produce very compa...  multi_context   \n",
       "10  DocLayNet's category distribution includes Fin...  multi_context   \n",
       "11  Object detection models, specifically a Mask R...  multi_context   \n",
       "12  Splitting DocLayNet affects Mask R-CNN R50 mAP...  multi_context   \n",
       "13  The DocLayNet dataset includes a variety of do...  multi_context   \n",
       "14  DocLayNet includes diverse and complex layouts...  multi_context   \n",
       "15  The answer to given question is not present in...  multi_context   \n",
       "16  The categories that cover Financial Reports an...      reasoning   \n",
       "17  BARSKA Model AY11236 is a powerful fixed power...      reasoning   \n",
       "18  The mAP score rises sharply in the beginning a...      reasoning   \n",
       "19  DocLayNet trained models yield better performa...      reasoning   \n",
       "\n",
       "                                             metadata  episode_done  \n",
       "0   [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "1   [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "2   [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "3   [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "4   [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "5   [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "6   [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "7   [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "8   [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "9   [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "10  [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "11  [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "12  [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "13  [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "14  [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "15  [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "16  [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "17  [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "18  [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  \n",
       "19  [{'source': 'https://arxiv.org/pdf/2206.01062'...          True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = \"DocLayNet7\"\n",
    "\n",
    "# dataset = client.create_dataset(\n",
    "#     dataset_name=dataset_name,\n",
    "#     description=title\n",
    "# )\n",
    "# for test in testset.to_pandas().iterrows():\n",
    "#   client.create_example(\n",
    "#       inputs={\n",
    "#           \"question\": test[1][\"question\"]\n",
    "#       },\n",
    "#       outputs={\n",
    "#           \"answer\": test[1][\"ground_truth\"]\n",
    "#       },\n",
    "#       metadata={\n",
    "#           \"context\": test[0]\n",
    "#       },\n",
    "#       dataset_id=dataset.id\n",
    "#   )\n",
    "\n",
    "  # What if the dataset already exists or dont want to rerun this again?\n",
    "# dataset1 = client.read_dataset(dataset_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7866bba0feda4803a50639b705b767fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pdfloader import PDFLoader\n",
    "\n",
    "pymupdf_loader = PDFLoader(FILE_PATH,PDFLoader.LoaderType.PYMUPDF)\n",
    "pymupdf4llm_loader = PDFLoader(FILE_PATH,PDFLoader.LoaderType.PYMUPDF4LLM)\n",
    "docling_loader = PDFLoader(FILE_PATH,PDFLoader.LoaderType.DOCLING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size=1000\n",
    "chunk_overlap=50\n",
    "chunking_strategy = \"RecursiveCharacterTextSplitter\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1 documents with LoaderType.PYMUPDF loader...\n",
      "Loading 1 documents with LoaderType.PYMUPDF4LLM loader...\n",
      "Loading PDF from /var/folders/fy/bc970z1n26gflr1xr_4kr5380000gn/T/tmp4f_or245.pdf...\n",
      "Processing /var/folders/fy/bc970z1n26gflr1xr_4kr5380000gn/T/tmp4f_or245.pdf...\n",
      "[                                        ] (0/9===[====                                    ] (1/9===[========                                ] (2/9====[=============                           ] (3/===[=================                       ] (4/====[======================                  ] (5/9===[==========================              ] (6/9====[===============================         ] (7/===[===================================     ] (8/====[========================================] (9/9]\n",
      "Loading 1 documents with LoaderType.DOCLING loader...\n"
     ]
    }
   ],
   "source": [
    "pymupdf_documents = pymupdf_loader.load()\n",
    "pymupdf4llm_documents = pymupdf4llm_loader.load()\n",
    "docling_documents = docling_loader.load()\n",
    "\n",
    "pymupdf_chunks = text_splitter.split_documents(pymupdf_documents)\n",
    "pymupdf4llm_chunks = text_splitter.split_documents(pymupdf4llm_documents)\n",
    "docling_chunks = text_splitter.split_documents(docling_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_modle_for_rag_chain = \"text-embedding-3-large\"\n",
    "embeddings = OpenAIEmbeddings(model=embedding_modle_for_rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "pymupdf_collection_name = \"pymupdf_collection\"\n",
    "pymupdf_vectorstore = Qdrant.from_documents(\n",
    "    documents=pymupdf_chunks,\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=pymupdf_collection_name\n",
    ")\n",
    "pymupdf_retriever = pymupdf_vectorstore.as_retriever()\n",
    "\n",
    "pymupdf4llm_collection_name = \"pymupdf4llm_collection\"\n",
    "pymupdf4llm_vectorstore = Qdrant.from_documents(\n",
    "    documents=pymupdf4llm_chunks,\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=pymupdf4llm_collection_name\n",
    ")\n",
    "pymupdf4llm_retriever = pymupdf4llm_vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "docling_collection_name = \"docling_collection\"\n",
    "docling_vectorstore = Qdrant.from_documents(\n",
    "    documents=docling_chunks,\n",
    "    embedding=embeddings,\n",
    "    location=\":memory:\",\n",
    "    collection_name=docling_collection_name\n",
    ")\n",
    "docling_retriever = docling_vectorstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'dl_doc_hash': '5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc', '_id': '9c36425fbc0b4b8581a2678710eb702f', '_collection_name': 'docling_collection'}, page_content='Articles category. We also avoided class labels that are tightly linked to the semantics of the text. Labels such as Author and Affiliation, as seen in DocBank, are often only distinguishable by discriminating on'),\n",
       " Document(metadata={'dl_doc_hash': '5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc', '_id': '98e9cbd286cd4e4092131a212b1be5e2', '_collection_name': 'docling_collection'}, page_content='were carried out over a timeframe of 12 weeks, after which 8 of the 40 initially allocated annotators did not pass the bar.'),\n",
       " Document(metadata={'dl_doc_hash': '5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc', '_id': 'c694ac5b90a749c7bb1a9858855c9aee', '_collection_name': 'docling_collection'}, page_content='out-performs humans on selected labels such as Text, t, Table and Picture. This is not entirely surprising, as Text, t, Table and Picture are abundant and the most visually distinctive in a document.'),\n",
       " Document(metadata={'dl_doc_hash': '5dfbd8c115a15fd3396b68409124cfee29fc8efac7b5c846634ff924e635e0dc', '_id': 'ab27eb4b1934430db44deb2a887cea42', '_collection_name': 'docling_collection'}, page_content=\"Phase 4: Production annotation. The previously selected 80K pages were annotated with the defined 11 class labels by 32 annotators. This production phase took around three months to complete. All annotations were created online through CCS, which visualises the programmatic PDF text-cells as an overlay on the page. The page annotation are obtained by drawing rectangular bounding-boxes, as shown in Figure 3. With regard to the annotation practices, we implemented a few constraints and capabilities on the tooling level. First, we only allow non-overlapping, vertically oriented, rectangular boxes. For the large majority of documents, this constraint was sufficient and it speeds up the annotation considerably in comparison with arbitrary segmentation shapes. Second, annotator staff were not able to see each other's annotations. This was enforced by design to avoid any bias in the annotation, which could skew the numbers of the inter-annotator agreement (see Table 1). We wanted\")]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docling_retriever.invoke(\"BARSK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "Given a provided context and question, you must answer the question based only on context.\n",
    "\n",
    "If you cannot answer the question based on the context - you must say \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_model_for_rag = \"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(model=llm_model_for_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "pymupdf_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | pymupdf_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "pymupdf4llm_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | pymupdf4llm_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "docling_rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | docling_retriever, \"question\": itemgetter(\"question\")}\n",
    "    | rag_prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DocLayNet provides several benefits for machine learning in layout analysis compared to other datasets like PubLayNet and DocBank. Firstly, it offers a wide variability in layouts, as it contains 80,863 manually annotated pages from diverse data sources, which helps improve the accuracy of layout predictions on different document types. Additionally, DocLayNet is human-annotated, which can enhance the quality of the annotations. It also includes a subset of double- and triple-annotated pages to assess inter-annotator agreement, allowing for better evaluation of the annotation quality. Lastly, it is formatted in COCO format, which is commonly accepted in the community, making it easier to use with existing machine learning frameworks.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymupdf_rag_chain.invoke({\"question\" : \"What benefits does DocLayNet provide for ML in layout analysis vs. other datasets?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DocLayNet provides several benefits for machine learning in layout analysis compared to other datasets:\\n\\n1. **Human Annotation**: Unlike PubLayNet and DocBank, DocLayNet relies on human annotation to generate the dataset, which ensures reliable layout ground-truth.\\n\\n2. **Large Layout Variability**: It includes diverse and complex layouts from a wide variety of public sources, adding significant value to the dataset.\\n\\n3. **Detailed Label Set**: DocLayNet defines 11 class labels for distinguishing layout features in high detail, which is more detailed compared to the 5 labels in PubLayNet.\\n\\n4. **Redundant Annotations**: A fraction of the pages in DocLayNet carries more than one human annotation, enabling experimentation with annotation uncertainty and quality control analysis.\\n\\nThese aspects contribute to improving and fine-tuning novel machine learning methods for document layout analysis.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pymupdf4llm_rag_chain.invoke({\"question\" : \"What benefits does DocLayNet provide for ML in layout analysis vs. other datasets?\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DocLayNet provides several benefits for machine learning in layout analysis compared to other datasets:\\n\\n1. **Human Annotation**: Unlike PubLayNet and DocBank, DocLayNet relies on human annotation instead of automated approaches, which can lead to higher quality data.\\n\\n2. **Large Layout Variability**: The dataset includes diverse and complex layouts from a wide variety of public sources, addressing the lack of layout variability found in other datasets that are sourced primarily from scientific article repositories.\\n\\n3. **Detailed Label Set**: DocLayNet defines 11 class labels to distinguish layout features in high detail, which is more detailed than the 5 labels provided by PubLayNet and offers a different focus than the 13 labels in DocBank.\\n\\n4. **Inter-Annotator Agreement**: DocLayNet includes a subset of double- and triple-annotated pages to determine inter-annotator agreement, providing a measure of reliability for the annotations.\\n\\n5. **Robust Model Performance**: Models trained on DocLayNet demonstrate more robust layout predictions compared to those trained on PubLayNet and DocBank, making them a preferred choice for general-purpose document-layout analysis.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docling_rag_chain.invoke({\"question\" : \"What benefits does DocLayNet provide for ML in layout analysis vs. other datasets?\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langsmith evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_as_judge = \"gpt-4o\"\n",
    "eval_llm = ChatOpenAI(model=llm_as_judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator\n",
    "\n",
    "qa_evaluator = LangChainStringEvaluator(\"qa\", config={\"llm\": eval_llm}) \n",
    "cot_qa_evaluator = LangChainStringEvaluator(\"cot_qa\",config={\"llm\": eval_llm})\n",
    "context_qa_evaluator = LangChainStringEvaluator(\"context_qa\", config={\"llm\": eval_llm})\n",
    "\n",
    "labeled_helpfulness_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_criteria\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"helpfulness\": (\n",
    "                \"Is this submission helpful to the user,\"\n",
    "                \" taking into account the correct reference answer?\"\n",
    "            )\n",
    "        }\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"output\"],\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "scoring_accuracy_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"accuracy\": \"Score 1: Completely inaccurate\\nScore 5: Somewhat accurate\\nScore 10: Completely accurate\"\n",
    "        },\n",
    "        \"normalize_by\": 10,\n",
    "        \"llm\": eval_llm,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"output\"],\n",
    "        \"reference\": example.outputs[\"answer\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'pymupdf-8a8b9611' at:\n",
      "https://smith.langchain.com/o/e319c8f1-73bd-5d44-8897-d19a191ebc54/datasets/c9a1d7bd-5aab-4a5a-be54-b2d049f5c0aa/compare?selectedSessions=24840705-4f1f-41a3-a474-f59d8c51bd06\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eecec844d37441a8f17190a32a15726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults pymupdf-8a8b9611>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "from langsmith import Client\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "client = Client(auto_batch_tracing=False)\n",
    "dist =str({simple: 0.4,multi_context: 0.4,reasoning: 0.2})\n",
    "evaluate(\n",
    "    pymupdf_rag_chain.invoke,\n",
    "    data=dataset_name,\n",
    "    experiment_prefix=\"pymupdf\",\n",
    "    max_concurrency=2,\n",
    "    client=client,\n",
    "    evaluators=[\n",
    "        qa_evaluator,\n",
    "        cot_qa_evaluator,\n",
    "        context_qa_evaluator,\n",
    "        labeled_helpfulness_evaluator,\n",
    "        scoring_accuracy_evaluator\n",
    "    ],\n",
    "    metadata={\n",
    "            \"chain\": \"pymupdf_rag_chain\",\n",
    "            \"dataset\": dataset_name,\n",
    "            \"title\": title,\n",
    "            \"generator_model\": llm_as_generator,\n",
    "            \"critic_model\": llm_as_critic,\n",
    "            \"embedding_model_generator\": embedding_model_for_generator,\n",
    "            \"embedding_model_dimensions\": str(embedding_model_dimensions),\n",
    "            \"distributions\": dist,\n",
    "            \"chunk_size\": str(chunk_size),\n",
    "            \"chunk_overlap\": str(chunk_overlap),\n",
    "            \"chunking_strategy\": chunking_strategy,\n",
    "            \"embedding_model_rag_chain\": embedding_modle_for_rag_chain,\n",
    "            \"llm_model_rag\": llm_model_for_rag,\n",
    "            \"llm_as_judge\": llm_as_judge            \n",
    "        },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'pymupdf4llm-6547e321' at:\n",
      "https://smith.langchain.com/o/e319c8f1-73bd-5d44-8897-d19a191ebc54/datasets/c9a1d7bd-5aab-4a5a-be54-b2d049f5c0aa/compare?selectedSessions=2ad051e3-f354-4e7d-b213-005e9e6ed52e\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45eae3f2df64a08a4b908bd6f710317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults pymupdf4llm-6547e321>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    pymupdf4llm_rag_chain.invoke,\n",
    "    data=dataset_name,\n",
    "    experiment_prefix=\"pymupdf4llm\",\n",
    "    max_concurrency=2,\n",
    "    client=client,\n",
    "    evaluators=[\n",
    "        qa_evaluator,\n",
    "        cot_qa_evaluator,\n",
    "        context_qa_evaluator,\n",
    "        labeled_helpfulness_evaluator,\n",
    "        scoring_accuracy_evaluator\n",
    "    ],\n",
    "    metadata={\n",
    "            \"chain\": \"pymupdf4llm_rag_chain\",\n",
    "            \"dataset\": dataset_name,\n",
    "            \"title\": title,\n",
    "            \"generator_model\": llm_as_generator,\n",
    "            \"critic_model\": llm_as_critic,\n",
    "            \"embedding_model_generator\": embedding_model_for_generator,\n",
    "            \"embedding_model_dimensions\": str(embedding_model_dimensions),\n",
    "            \"distributions\": dist,\n",
    "            \"chunk_size\": str(chunk_size),\n",
    "            \"chunk_overlap\": str(chunk_overlap),\n",
    "            \"chunking_strategy\": chunking_strategy,\n",
    "            \"embedding_model_rag_chain\": embedding_modle_for_rag_chain,\n",
    "            \"llm_model_rag\": llm_model_for_rag,\n",
    "            \"llm_as_judge\": llm_as_judge            \n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'docling-9db64e5f' at:\n",
      "https://smith.langchain.com/o/e319c8f1-73bd-5d44-8897-d19a191ebc54/datasets/c9a1d7bd-5aab-4a5a-be54-b2d049f5c0aa/compare?selectedSessions=efcb1c8c-ae18-4e90-8fd9-239a1b89fc1f\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b007d53caa4c2e81a442c7ab8e6677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults docling-9db64e5f>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    docling_rag_chain.invoke,\n",
    "    data=dataset_name,\n",
    "    experiment_prefix=\"docling\",\n",
    "    max_concurrency=2,\n",
    "    client=client,\n",
    "    evaluators=[\n",
    "        qa_evaluator,\n",
    "        cot_qa_evaluator,\n",
    "        context_qa_evaluator,\n",
    "        labeled_helpfulness_evaluator,\n",
    "        scoring_accuracy_evaluator\n",
    "    ],\n",
    "    metadata={\n",
    "            \"chain\": \"docling_rag_chain\",\n",
    "            \"dataset\": dataset_name,\n",
    "            \"title\": title,\n",
    "            \"generator_model\": llm_as_generator,\n",
    "            \"critic_model\": llm_as_critic,\n",
    "            \"embedding_model_generator\": embedding_model_for_generator,\n",
    "            \"embedding_model_dimensions\": str(embedding_model_dimensions),\n",
    "            \"distributions\": dist,\n",
    "            \"chunk_size\": str(chunk_size),\n",
    "            \"chunk_overlap\": str(chunk_overlap),\n",
    "            \"chunking_strategy\": chunking_strategy,\n",
    "            \"embedding_model_rag_chain\": embedding_modle_for_rag_chain,\n",
    "            \"llm_model_rag\": llm_model_for_rag,\n",
    "            \"llm_as_judge\": llm_as_judge            \n",
    "        },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name: DocLayNet7\n",
      "title: DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis\n",
      "generator_model: gpt-4o\n",
      "critic_model: gpt-4o\n",
      "embedding_model_generator: text-embedding-3-large\n",
      "embedding_model_dimensions: 3072\n",
      "chunk_size:1000\n",
      "chunk_overlap:50\n",
      "chunking_strategy: RecursiveCharacterTextSplitter\n",
      "embedding_model_rag_chain: text-embedding-3-large\n",
      "llm_model_rag: gpt-4o-mini\n",
      "llm_as_judge: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "dist =str({simple: 0.4,multi_context: 0.4,reasoning: 0.2})\n",
    "\n",
    "print(\"dataset_name: \" + dataset_name)\n",
    "print(\"title: \"+ title)\n",
    "print(\"generator_model: \"+ llm_as_generator)\n",
    "print(\"critic_model: \"+ llm_as_critic)\n",
    "print(\"embedding_model_generator: \"+ embedding_model_for_generator)\n",
    "print(\"embedding_model_dimensions: \"+ str(embedding_model_dimensions))\n",
    "print(\"chunk_size:\"+ str(chunk_size))\n",
    "print(\"chunk_overlap:\"+ str(chunk_overlap))\n",
    "print(\"chunking_strategy: \"+ chunking_strategy)\n",
    "print(\"embedding_model_rag_chain: \"+ embedding_modle_for_rag_chain)\n",
    "print(\"llm_model_rag: \"+ llm_model_for_rag)\n",
    "print(\"llm_as_judge: \"+ llm_as_judge)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
